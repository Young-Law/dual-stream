{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18bc6e20",
   "metadata": {},
   "source": [
    "# Google Tunix Hackathon: Dual-Stream End-to-End Kernel\n",
    "\n",
    "This notebook is structured for Kaggle submission in the Google Tunix hackathon. It runs entirely from the checked-in repo without external assets, and it reproduces the full Dual-Stream Architecture plus the `dualstream_anticollapse` monitoring toolkit shipped in this repository. Key compliance notes:\n",
    "\n",
    "- ✅ Self contained: installs only from local requirements and Hugging Face models that Kaggle already caches.\n",
    "- ✅ Deterministic seeds and CPU-friendly defaults so it runs inside the kernel limits.\n",
    "- ✅ Clear outputs and saved artifacts that match the competition's expectation for a single-run, reviewable notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1c86cf",
   "metadata": {},
   "source": [
    "## 1) Environment prep\n",
    "\n",
    "The cell below auto-detects the repo location (works on Kaggle or locally), installs minimal dependencies, and sets seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1d976c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T19:13:27.609080Z",
     "iopub.status.busy": "2025-11-26T19:13:27.608766Z",
     "iopub.status.idle": "2025-11-26T19:13:32.395173Z",
     "shell.execute_reply": "2025-11-26T19:13:32.393956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using repo root: /workspace/dual-stream\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, sys, json, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Detect repository root (works on Kaggle datasets and local clones)\n",
    "DEFAULT_ROOT = Path('.')\n",
    "kaggle_root = Path('/kaggle/input/dual-stream')\n",
    "repo_root = kaggle_root if kaggle_root.exists() else DEFAULT_ROOT\n",
    "print(f\"Using repo root: {repo_root.resolve()}\")\n",
    "\n",
    "# Make project modules importable\n",
    "sys.path.append(str(repo_root / 'python_poc'))\n",
    "sys.path.append(str(repo_root / 'dualstream_anticollapse'))\n",
    "\n",
    "# Light-weight dependency install (no custom wheels needed)\n",
    "try:\n",
    "    import pandas  # noqa: F401\n",
    "except ImportError:\n",
    "    !pip install -q pandas scikit-learn scipy matplotlib seaborn\n",
    "\n",
    "# Transformers/torch are large; install only if missing.\n",
    "try:\n",
    "    import transformers  # noqa: F401\n",
    "except ImportError:\n",
    "    !pip install -q -r {repo_root}/python_poc/requirements.txt\n",
    "\n",
    "# Seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6412ec91",
   "metadata": {},
   "source": [
    "## 2) Dual-Stream generator\n",
    "\n",
    "Wrap the `python_poc/dual_stream_poc.py` implementation so we can produce Answer + Monologue streams directly from the notebook. To stay within Kaggle limits the default model is `gpt2`, but you can swap in a Tunix-supplied checkpoint by changing `model_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e9f4b36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T19:13:32.398133Z",
     "iopub.status.busy": "2025-11-26T19:13:32.397672Z",
     "iopub.status.idle": "2025-11-26T19:13:41.520175Z",
     "shell.execute_reply": "2025-11-26T19:13:41.518542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer Stream:\n",
      " \n",
      "\n",
      "The first thing to understand is that model reasoning is a very complex process. It is not a simple process. It is a complex process that requires a lot of work.\n",
      "\n",
      "The second\n",
      "\n",
      "Monologue Stream (first 5 lines):\n",
      "[LOGIT_LENS:TOP_5:('198',0.238),('Explain',0.036),('If',0.032),('You',0.030),('The',0.026)]\n",
      "[LOGIT_LENS:TOP_5:('198',0.998),('The',0.000),('1',0.000),('A',0.000),('I',0.000)]\n",
      "[LOGIT_LENS:TOP_5:('The',0.045),('What',0.028),('1',0.027),('In',0.021),('',0.021)]\n",
      "[LOGIT_LENS:TOP_5:('first',0.038),('model',0.027),('following',0.024),('most',0.014),('problem',0.013)]\n",
      "[LOGIT_LENS:TOP_5:('thing',0.226),('step',0.163),('question',0.050),('part',0.033),('rule',0.030)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from dual_stream_poc import DualStream\n",
    "\n",
    "prompt = \"\"\"You are an alignment auditor. Briefly explain why transparency in model reasoning matters.\"\"\"\n",
    "ds = DualStream(model_name=\"gpt2\", top_k=5, device=device)\n",
    "dual_result = ds.generate(prompt=prompt, max_new_tokens=40, temperature=0.0)\n",
    "\n",
    "print(\"Answer Stream:\\n\", dual_result['answer_text'])\n",
    "print(\"\\nMonologue Stream (first 5 lines):\")\n",
    "for line in dual_result['monologue_text'].split('\\n')[:5]:\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbda5853",
   "metadata": {},
   "source": [
    "## 3) Train baseline and monitor for drift/performance regressions\n",
    "\n",
    "`dualstream_anticollapse` ships drift detection, performance checks, and governance helpers. The next cell mirrors the CLI flow (`train` ➜ `monitor`) using the bundled demo CSVs, producing on-disk artifacts compatible with the hackathon review process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5623363",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T19:13:41.524051Z",
     "iopub.status.busy": "2025-11-26T19:13:41.523365Z",
     "iopub.status.idle": "2025-11-26T19:13:41.645694Z",
     "shell.execute_reply": "2025-11-26T19:13:41.644171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline metrics {\n",
      "  \"accuracy\": 0.826,\n",
      "  \"precision\": 0.7800687285223368,\n",
      "  \"recall\": 0.908,\n",
      "  \"f1\": 0.8391866913123845,\n",
      "  \"confusion_matrix\": [\n",
      "    [\n",
      "      186,\n",
      "      64\n",
      "    ],\n",
      "    [\n",
      "      23,\n",
      "      227\n",
      "    ]\n",
      "  ],\n",
      "  \"auc\": 0.9235680000000001,\n",
      "  \"log_loss\": 1.0177470322299007\n",
      "}\n",
      "{\"ts\": 1764184421.6330183, \"event\": \"data_drift\", \"payload\": {\"drifted\": [{\"feature\": \"x1\", \"psi\": 0.34730059968180615, \"ks_pvalue\": 1.4169802610765143e-14}]}}\n",
      "{\"ts\": 1764184421.6410162, \"event\": \"performance_degradation\", \"payload\": {\"triggers\": [\"accuracy_drop\"], \"metrics\": {\"accuracy\": 0.742, \"precision\": 0.6809651474530831, \"recall\": 0.9621212121212122, \"f1\": 0.7974882260596546, \"confusion_matrix\": [[117, 119], [10, 254]], \"auc\": 0.8975988700564972, \"log_loss\": 2.543059662289643}, \"baseline\": {\"accuracy\": 0.826, \"precision\": 0.7800687285223368, \"recall\": 0.908, \"f1\": 0.8391866913123845, \"confusion_matrix\": [[186, 64], [23, 227]], \"auc\": 0.9235680000000001, \"log_loss\": 1.0177470322299007}}}\n",
      "Drift triggered: True\n",
      "Outliers triggered: False\n",
      "Performance report:\n",
      " true\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from dualstream_anticollapse.config import Config\n",
    "from dualstream_anticollapse.retrain import build_model, fit_model, predict\n",
    "from dualstream_anticollapse.metrics import classification_metrics\n",
    "from dualstream_anticollapse.governance import save_model\n",
    "from dualstream_anticollapse.monitor import ModelMonitor\n",
    "\n",
    "artifacts = Path('artifacts')\n",
    "artifacts.mkdir(exist_ok=True)\n",
    "\n",
    "# Load demo data\n",
    "ref_path = repo_root / 'dualstream_anticollapse' / 'demo' / 'reference.csv'\n",
    "cur_path = repo_root / 'dualstream_anticollapse' / 'demo' / 'current.csv'\n",
    "ref = pd.read_csv(ref_path)\n",
    "cur = pd.read_csv(cur_path)\n",
    "\n",
    "# Fit a baseline classifier\n",
    "target = 'y'\n",
    "features = ['x1', 'x2']\n",
    "cfg = Config(target=target, features=features, output_dir=str(artifacts))\n",
    "\n",
    "X_ref, y_ref = ref[features], ref[target].astype(int)\n",
    "model = build_model(cfg.model_type)\n",
    "model = fit_model(model, X_ref, y_ref)\n",
    "\n",
    "# Save baseline metrics + model artifacts\n",
    "y_pred_ref, y_proba_ref = predict(model, X_ref)\n",
    "metrics_base = classification_metrics(y_ref, y_pred_ref, y_proba_ref)\n",
    "save_model(model, artifacts / 'model.joblib', {\"stage\": \"baseline\", \"metrics\": metrics_base})\n",
    "print(\"Baseline metrics\", json.dumps(metrics_base, indent=2))\n",
    "\n",
    "# Monitor a new batch\n",
    "mon = ModelMonitor(cfg, {\"metrics\": metrics_base}, state_path=str(artifacts / 'state.json'), alert_sink='stdout')\n",
    "drift_triggered = mon.check_drift(ref, cur)\n",
    "outliers_triggered, outliers = mon.check_outliers(cur)\n",
    "\n",
    "X_cur, y_cur = cur[features], cur[target].astype(int)\n",
    "y_pred_cur, y_proba_cur = predict(model, X_cur)\n",
    "perf_report = mon.check_performance(classification_metrics(y_cur, y_pred_cur, y_proba_cur))\n",
    "\n",
    "print(\"Drift triggered:\", drift_triggered)\n",
    "print(\"Outliers triggered:\", outliers_triggered)\n",
    "print(\"Performance report:\\n\", json.dumps(perf_report, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc40aa8",
   "metadata": {},
   "source": [
    "## 4) Coherence audit of the Dual-Stream output\n",
    "\n",
    "Convert the generated Answer/Monologue into the JSONL schema expected by the Coherence Auditor and run the audit. Any misalignment or safety markers will be surfaced in `report`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "087b3db8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T19:13:41.648646Z",
     "iopub.status.busy": "2025-11-26T19:13:41.648351Z",
     "iopub.status.idle": "2025-11-26T19:13:41.656443Z",
     "shell.execute_reply": "2025-11-26T19:13:41.654996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"answer\": \"\\n\\nThe first thing to understand is that model reasoning is a very complex process. It is not a simple process. It is a complex process that requires a lot of work.\\n\\nThe second\",\n",
      "  \"coherent\": true,\n",
      "  \"reasons\": [],\n",
      "  \"deception_hits\": [],\n",
      "  \"conflict_hits\": [],\n",
      "  \"safety_hits\": [],\n",
      "  \"logits_topk\": [\n",
      "    [\n",
      "      \"\",\n",
      "      0.23849539458751678\n",
      "    ],\n",
      "    [\n",
      "      \"Explain\",\n",
      "      0.036299243569374084\n",
      "    ],\n",
      "    [\n",
      "      \"If\",\n",
      "      0.032072119414806366\n",
      "    ],\n",
      "    [\n",
      "      \"You\",\n",
      "      0.029976723715662956\n",
      "    ],\n",
      "    [\n",
      "      \"The\",\n",
      "      0.026129810139536858\n",
      "    ]\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from dualstream_anticollapse.coherence import CoherenceAuditor\n",
    "\n",
    "# Assemble a JSONL-style record from the DualStream output\n",
    "def build_logit_topk(frames):\n",
    "    if not frames:\n",
    "        return []\n",
    "    # Use the top-k distribution from the first generation step as a summary\n",
    "    first = frames[0]\n",
    "    tokenizer = ds.tokenizer\n",
    "    return [(tokenizer.decode([tid]).strip(), prob) for tid, prob in zip(first['topk_ids'], first['topk_probs'])]\n",
    "\n",
    "record = {\n",
    "    \"answer\": dual_result['answer_text'],\n",
    "    \"monologue\": dual_result['monologue_text'],\n",
    "    \"logits_topk\": build_logit_topk(dual_result['monologue_frames']),\n",
    "}\n",
    "\n",
    "# Run audit\n",
    "ca = CoherenceAuditor(cfg.thresholds.__dict__)\n",
    "report = ca.audit_record(record)\n",
    "print(json.dumps(report, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e80ad60",
   "metadata": {},
   "source": [
    "## 5) Export for Kaggle submission\n",
    "\n",
    "The following cell writes the monitored artifacts and the coherence report to the working directory. On Kaggle, mark these files as output so reviewers can verify compliance with the Tunix rules without re-running heavy models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "487aaa50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T19:13:41.659835Z",
     "iopub.status.busy": "2025-11-26T19:13:41.659539Z",
     "iopub.status.idle": "2025-11-26T19:13:41.670057Z",
     "shell.execute_reply": "2025-11-26T19:13:41.668719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: ['coherence_report.json', 'dual_stream_output.json']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_dir = Path('submission_outputs')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save coherence report\n",
    "with open(output_dir / 'coherence_report.json', 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "# Save dual-stream output\n",
    "with open(output_dir / 'dual_stream_output.json', 'w') as f:\n",
    "    json.dump(dual_result, f, indent=2)\n",
    "\n",
    "print(\"Wrote:\", sorted(p.name for p in output_dir.iterdir()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05804a7e",
   "metadata": {},
   "source": [
    "### What to submit\n",
    "\n",
    "- The notebook itself (this file) with all cells executed.\n",
    "- The `submission_outputs/` folder containing `coherence_report.json` and `dual_stream_output.json` as supplemental artifacts.\n",
    "- Optionally the `artifacts/` folder if you want reviewers to inspect drift/performance traces.\n",
    "\n",
    "This layout matches Kaggle's single-notebook review flow and demonstrates the full integration of the Dual-Stream generator plus the anti-collapse/monitoring stack."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
