{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20ab78be",
   "metadata": {},
   "source": [
    "# DSA-CAST + Tunix GRPO on Gemma3-1B (TPU, Kaggle)\n",
    "\n",
    "This notebook:\n",
    "\n",
    "1. Sets up **Gemma3-1B-IT** on a Kaggle TPU using **Tunix**.\n",
    "2. Uses the `<reasoning> ... </reasoning>` and `<answer> ... </answer>` format for math problems (GSM8K-style).\n",
    "3. Defines a **CAST-style reward** that strongly favors:\n",
    "   - mathematical accuracy, and  \n",
    "   - answer completeness & proper tagging.\n",
    "4. Runs a **Tunix GRPO** reinforcement learning loop using that reward.\n",
    "5. Saves the final **Tunix checkpoint (no safetensors export)** so it can be re-used in another notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1afc729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Environment setup: JAX TPU + Tunix + Gemma ===\n",
    "import os, random, numpy as np\n",
    "\n",
    "# Use TPU memory efficiently\n",
    "os.environ.setdefault(\"JAX_PLATFORMS\", \"tpu,cpu\")\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"\n",
    "\n",
    "# Install JAX (TPU build), Tunix, Gemma, and helpers\n",
    "!pip install -q \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
    "!pip install -q tunix gemma qwix datasets humanize tensorflow_datasets kagglehub\n",
    "\n",
    "print(\"Environment installs complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb31592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports & global configuration ===\n",
    "import functools\n",
    "import json\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "import optax\n",
    "import humanize\n",
    "import grain\n",
    "import tensorflow_datasets as tfds\n",
    "import kagglehub\n",
    "from orbax import checkpoint as ocp\n",
    "\n",
    "from tunix.generate import sampler as sampler_lib\n",
    "from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
    "from tunix.models.gemma3 import model as gemma3_tunix_model\n",
    "from tunix.models.gemma3 import params_safetensors as gemma3_params_sft\n",
    "from tunix.models.gemma3 import params as gemma3_params_lib\n",
    "from tunix.rl import rl_cluster as rl_cluster_lib\n",
    "from tunix.rl.grpo.grpo_learner import GRPOLearner, GRPOConfig\n",
    "from tunix.rl.rollout import base_rollout\n",
    "from tunix.sft import metrics_logger\n",
    "\n",
    "print(\"Imported core libraries.\")\n",
    "\n",
    "# Random seeds\n",
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)\n",
    "jax_key = jax.random.PRNGKey(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Special tags and system prompt\n",
    "REASONING_START = \"<reasoning>\"\n",
    "REASONING_END = \"</reasoning>\"\n",
    "ANSWER_START = \"<answer>\"\n",
    "ANSWER_END = \"</answer>\"\n",
    "\n",
    "SYSTEM_PROMPT = f\"\"\"You are a careful math tutor.\n",
    "\n",
    "For each question:\n",
    "- First, think through the problem step by step.\n",
    "- Put all of your step-by-step reasoning strictly between {REASONING_START} and {REASONING_END}.\n",
    "- Then, put the final numeric answer (only the number) strictly between {ANSWER_START} and {ANSWER_END}.\n",
    "\n",
    "You MUST include both blocks, in this order.\n",
    "\"\"\".strip()\n",
    "\n",
    "TEMPLATE = \"\"\"<start_of_turn>user\n",
    "{system_prompt}\n",
    "\n",
    "{question}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea58b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Hyperparameters ===\n",
    "\n",
    "MODEL_ID = \"google/gemma-3-1b-it\"\n",
    "\n",
    "TRAIN_DATA_DIR = \"./data/gsm8k_train\"\n",
    "TEST_DATA_DIR = \"./data/gsm8k_test\"\n",
    "\n",
    "NUM_TPUS = len(jax.devices())\n",
    "if NUM_TPUS == 8:\n",
    "    MESH_COUNTS = (1, 4)\n",
    "elif NUM_TPUS == 1:\n",
    "    MESH_COUNTS = (1, 1)\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported number of TPU devices: {NUM_TPUS}\")\n",
    "\n",
    "MESH = [MESH_COUNTS, (\"fsdp\", \"tp\")]\n",
    "\n",
    "MAX_PROMPT_LENGTH = 256\n",
    "TOTAL_GENERATION_STEPS = 384\n",
    "TEMPERATURE = 0.9\n",
    "TOP_P = 1.0\n",
    "TOP_K = 50\n",
    "NUM_GENERATIONS = 2\n",
    "NUM_ITERATIONS = 1\n",
    "\n",
    "TRAIN_MICRO_BATCH_SIZE = 1\n",
    "NUM_BATCHES = 256\n",
    "TRAIN_FRACTION = 0.9\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "MAX_STEPS = int(NUM_BATCHES * NUM_ITERATIONS * TRAIN_FRACTION * NUM_EPOCHS)\n",
    "\n",
    "LEARNING_RATE = 3e-6\n",
    "B1 = 0.9\n",
    "B2 = 0.99\n",
    "WEIGHT_DECAY = 0.1\n",
    "WARMUP_STEPS = int(0.1 * MAX_STEPS)\n",
    "MAX_GRAD_NORM = 0.1\n",
    "\n",
    "CKPT_DIR = \"/kaggle/working/grpo_ckpts\"\n",
    "SAVE_INTERVAL_STEPS = 200\n",
    "MAX_TO_KEEP = 4\n",
    "\n",
    "GENERATION_CONFIGS = {\n",
    "    \"greedy\":   {\"temperature\": None, \"top_k\": 1,   \"top_p\": None},\n",
    "    \"standard\": {\"temperature\": 0.7,  \"top_k\": 50,  \"top_p\": 0.95},\n",
    "    \"liberal\":  {\"temperature\": 0.85, \"top_k\": 2000,\"top_p\": 1.0},\n",
    "}\n",
    "\n",
    "print(\"Hyperparameters set. MAX_STEPS =\", MAX_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c464ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Data preprocessing: GSM8K via TFDS ===\n",
    "\n",
    "def extract_hash_answer(text: str) -> Optional[str]:\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\", 1)[1].strip()\n",
    "\n",
    "def _load_gsm8k_tfds(data_dir: str, split: str):\n",
    "    import tensorflow_datasets.text.gsm8k\n",
    "    return tfds.data_source(\n",
    "        \"gsm8k\",\n",
    "        split=split,\n",
    "        data_dir=data_dir,\n",
    "        builder_kwargs={\"file_format\": tfds.core.FileFormat.ARRAY_RECORD},\n",
    "        download=True,\n",
    "    )\n",
    "\n",
    "def get_gsm8k_dataset(data_dir: str, split: str = \"train\") -> grain.MapDataset:\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    ds = _load_gsm8k_tfds(data_dir, split)\n",
    "\n",
    "    def _as_text(v):\n",
    "        return v if isinstance(v, str) else v.decode(\"utf-8\")\n",
    "\n",
    "    dataset = (\n",
    "        grain.MapDataset.source(ds)\n",
    "        .shuffle(seed=SEED)\n",
    "        .map(\n",
    "            lambda x: {\n",
    "                \"prompts\": TEMPLATE.format(\n",
    "                    system_prompt=SYSTEM_PROMPT,\n",
    "                    question=_as_text(x[\"question\"]),\n",
    "                ),\n",
    "                \"question\": _as_text(x[\"question\"]),\n",
    "                \"answer\": extract_hash_answer(_as_text(x[\"answer\"])),\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "train_raw = get_gsm8k_dataset(TRAIN_DATA_DIR, split=\"train\")\n",
    "test_raw = get_gsm8k_dataset(TEST_DATA_DIR, split=\"test\")\n",
    "\n",
    "train_dataset = train_raw.batch(TRAIN_MICRO_BATCH_SIZE)[:NUM_BATCHES]\n",
    "\n",
    "if TRAIN_FRACTION == 1.0:\n",
    "    train_dataset = train_dataset.repeat(NUM_EPOCHS)\n",
    "    val_dataset = None\n",
    "else:\n",
    "    cutoff = int(len(train_dataset) * TRAIN_FRACTION)\n",
    "    train_dataset = train_dataset[:cutoff].repeat(NUM_EPOCHS)\n",
    "    val_dataset = train_dataset[cutoff:].repeat(NUM_EPOCHS) if cutoff < len(train_dataset) else None\n",
    "\n",
    "NUM_TEST_BATCHES = 64\n",
    "test_dataset = test_raw.batch(TRAIN_MICRO_BATCH_SIZE)[:NUM_TEST_BATCHES]\n",
    "\n",
    "print(\"Dataset sizes (batches):\",\n",
    "      len(train_dataset),\n",
    "      0 if val_dataset is None else len(val_dataset),\n",
    "      len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54112c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Utility: TPU memory usage ===\n",
    "def show_hbm_usage():\n",
    "    fmt = functools.partial(humanize.naturalsize, binary=True)\n",
    "    for d in jax.local_devices():\n",
    "        stats = d.memory_stats()\n",
    "        used = stats[\"bytes_in_use\"]\n",
    "        limit = stats[\"bytes_limit\"]\n",
    "        print(f\"Using {fmt(used)} / {fmt(limit)} ({used/limit:%}) on {d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c1c8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load Gemma3-1B-IT with Tunix model wrappers ===\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "print(f\"Downloading {MODEL_ID} from Hugging Face (you must have access)...\")\n",
    "local_model_path = snapshot_download(repo_id=MODEL_ID, ignore_patterns=[\"*.pth\"])\n",
    "print(\"Local model path:\", local_model_path)\n",
    "\n",
    "model_config = gemma3_tunix_model.ModelConfig.gemma3_1b()\n",
    "\n",
    "mesh = jax.make_mesh(*MESH, axis_types=(jax.sharding.AxisType.Auto,) * len(MESH[0]))\n",
    "\n",
    "with mesh:\n",
    "    base_gemma = gemma3_params_sft.create_model_from_safe_tensors(\n",
    "        local_model_path,\n",
    "        model_config,\n",
    "        mesh,\n",
    "    )\n",
    "    nnx.display(base_gemma)\n",
    "\n",
    "import qwix\n",
    "\n",
    "RANK = 64\n",
    "ALPHA = 64.0\n",
    "\n",
    "def get_lora_model(base_model, mesh):\n",
    "    lora_provider = qwix.LoraProvider(\n",
    "        module_path=(\n",
    "            \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|.*attn_vec_einsum\"\n",
    "        ),\n",
    "        rank=RANK,\n",
    "        alpha=ALPHA,\n",
    "    )\n",
    "    model_input = base_model.get_model_input()\n",
    "    lora_model = qwix.apply_lora_to_model(base_model, lora_provider, **model_input)\n",
    "    with mesh:\n",
    "        state = nnx.state(lora_model)\n",
    "        pspecs = nnx.get_partition_spec(state)\n",
    "        sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
    "        nnx.update(lora_model, sharded_state)\n",
    "    return lora_model\n",
    "\n",
    "with mesh:\n",
    "    lora_policy = get_lora_model(base_gemma, mesh=mesh)\n",
    "    nnx.display(lora_policy)\n",
    "\n",
    "tokenizer = tokenizer_lib.Tokenizer.from_hf_hub(model_id=MODEL_ID)\n",
    "EOS_TOKENS = []\n",
    "gen_cfg_path = os.path.join(local_model_path, \"generation_config.json\")\n",
    "if os.path.exists(gen_cfg_path):\n",
    "    with open(gen_cfg_path, \"r\") as f:\n",
    "        gen_cfg = json.load(f)\n",
    "    eos_ids = gen_cfg.get(\"eos_token_id\", [])\n",
    "    if isinstance(eos_ids, int):\n",
    "        eos_ids = [eos_ids]\n",
    "    EOS_TOKENS.extend(eos_ids)\n",
    "\n",
    "if tokenizer.eos_id() not in EOS_TOKENS:\n",
    "    EOS_TOKENS.append(tokenizer.eos_id())\n",
    "\n",
    "print(\"EOS token IDs:\", EOS_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d195c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CAST-style helpers ===\n",
    "\n",
    "def extract_final_number(text: str) -> Optional[str]:\n",
    "    if text is None:\n",
    "        return None\n",
    "\n",
    "    m = re.search(r\"<answer>(.*?)</answer>\", text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    segment = m.group(1) if m else text\n",
    "\n",
    "    m = re.search(r\"####\\s*([-+]?[0-9][0-9.,/]*)\", segment)\n",
    "    if m:\n",
    "        return m.group(1).replace(\",\", \"\").strip()\n",
    "\n",
    "    nums = re.findall(r\"[-+]?[0-9][0-9.,/]*\", segment)\n",
    "    if not nums:\n",
    "        return None\n",
    "    return nums[-1].replace(\",\", \"\").strip()\n",
    "\n",
    "\n",
    "def cast_style_scores(\n",
    "    completions: List[str],\n",
    "    answers: List[Optional[str]],\n",
    ") -> Tuple[List[float], List[float], List[float]]:\n",
    "    math_accs: List[float] = []\n",
    "    completeness: List[float] = []\n",
    "    format_bonus: List[float] = []\n",
    "\n",
    "    for completion, target in zip(completions, answers):\n",
    "        text = completion or \"\"\n",
    "        target_text = target or \"\"\n",
    "\n",
    "        model_ans = extract_final_number(text)\n",
    "        target_ans = extract_final_number(target_text)\n",
    "        m_acc = 1.0 if (\n",
    "            model_ans is not None\n",
    "            and target_ans is not None\n",
    "            and model_ans == target_ans\n",
    "        ) else 0.0\n",
    "\n",
    "        lower = text.lower()\n",
    "        has_tags = (\"<reasoning\" in lower) and (\"<answer\" in lower)\n",
    "\n",
    "        reasoning_match = re.search(\n",
    "            r\"<reasoning>(.*?)</reasoning>\", text, flags=re.IGNORECASE | re.DOTALL\n",
    "        )\n",
    "        answer_match = re.search(\n",
    "            r\"<answer>(.*?)</answer>\", text, flags=re.IGNORECASE | re.DOTALL\n",
    "        )\n",
    "\n",
    "        reasoning_len = len(reasoning_match.group(1).strip()) if reasoning_match else 0\n",
    "        answer_len = len(answer_match.group(1).strip()) if answer_match else 0\n",
    "        total_len = reasoning_len + answer_len\n",
    "\n",
    "        if has_tags and total_len > 0:\n",
    "            c_score = min(1.0, total_len / 200.0)\n",
    "            f_bonus = 1.0\n",
    "        else:\n",
    "            c_score = 0.0\n",
    "            f_bonus = 0.0\n",
    "\n",
    "        math_accs.append(float(m_acc))\n",
    "        completeness.append(float(c_score))\n",
    "        format_bonus.append(float(f_bonus))\n",
    "\n",
    "    return math_accs, completeness, format_bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f08bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Reward functions for Tunix GRPO ===\n",
    "\n",
    "match_format = re.compile(\n",
    "    rf\"^[\\s]{{0,}}\"\n",
    "    rf\"{re.escape(REASONING_START)}.+?{re.escape(REASONING_END)}.*?\"\n",
    "    rf\"{re.escape(ANSWER_START)}(.+?){re.escape(ANSWER_END)}\"\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags=re.MULTILINE | re.DOTALL,\n",
    ")\n",
    "\n",
    "def reward_format_exact(prompts, completions, **kwargs):\n",
    "    scores = []\n",
    "    for resp in completions:\n",
    "        scores.append(2.0 if match_format.search(resp or \"\") else 0.0)\n",
    "    return scores\n",
    "\n",
    "def reward_format_soft(prompts, completions, **kwargs):\n",
    "    scores = []\n",
    "    for resp in completions:\n",
    "        r = 0.0\n",
    "        text = resp or \"\"\n",
    "        r += 0.5 if text.count(REASONING_START) == 1 else -0.5\n",
    "        r += 0.5 if text.count(REASONING_END) == 1 else -0.5\n",
    "        r += 0.5 if text.count(ANSWER_START) == 1 else -0.5\n",
    "        r += 0.5 if text.count(ANSWER_END) == 1 else -0.5\n",
    "        scores.append(r)\n",
    "    return scores\n",
    "\n",
    "def reward_cast_math_and_completeness(prompts, completions, answer, **kwargs):\n",
    "    math_accs, completeness, fbonus = cast_style_scores(completions, answer)\n",
    "    scores = []\n",
    "    for ma, c, fb in zip(math_accs, completeness, fbonus):\n",
    "        scores.append(3.0 * ma + 2.0 * c + 1.0 * fb)\n",
    "    return scores\n",
    "\n",
    "print(\"Reward functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5471ba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Evaluation utilities ===\n",
    "\n",
    "def build_sampler(policy_model, tokenizer, model_config):\n",
    "    return sampler_lib.Sampler(\n",
    "        transformer=policy_model,\n",
    "        tokenizer=tokenizer,\n",
    "        cache_config=sampler_lib.CacheConfig(\n",
    "            cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
    "            num_layers=model_config.num_layers,\n",
    "            num_kv_heads=model_config.num_kv_heads,\n",
    "            head_dim=model_config.head_dim,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_answers(questions, sampler, temperature=0.7, top_k=50, top_p=0.95, seed=None):\n",
    "    if isinstance(questions, str):\n",
    "        batch = [\n",
    "            TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=questions),\n",
    "        ]\n",
    "    else:\n",
    "        batch = [\n",
    "            TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=q)\n",
    "            for q in questions\n",
    "        ]\n",
    "    out = sampler(\n",
    "        input_strings=batch,\n",
    "        max_generation_steps=TOTAL_GENERATION_STEPS,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        echo=False,\n",
    "        seed=seed,\n",
    "        eos_tokens=EOS_TOKENS,\n",
    "    )\n",
    "    texts = out.text\n",
    "    return texts[0] if isinstance(questions, str) else texts\n",
    "\n",
    "\n",
    "def evaluate_dataset(dataset, sampler, num_passes=1):\n",
    "    total = 0\n",
    "    strict_correct = 0\n",
    "    approx_correct = 0\n",
    "    format_ok = 0\n",
    "\n",
    "    for batch in dataset:\n",
    "        questions = batch[\"question\"]\n",
    "        answers = batch[\"answer\"]\n",
    "        multiple_outputs = [[] for _ in range(len(questions))]\n",
    "\n",
    "        for s in range(num_passes):\n",
    "            responses = generate_answers(\n",
    "                questions,\n",
    "                sampler,\n",
    "                temperature=GENERATION_CONFIGS[\"greedy\"][\"temperature\"],\n",
    "                top_k=GENERATION_CONFIGS[\"greedy\"][\"top_k\"],\n",
    "                top_p=GENERATION_CONFIGS[\"greedy\"][\"top_p\"],\n",
    "                seed=s,\n",
    "            )\n",
    "            for idx, resp in enumerate(responses):\n",
    "                multiple_outputs[idx].append(resp)\n",
    "\n",
    "        for q, a, resp_list in zip(questions, answers, multiple_outputs):\n",
    "            is_correct = False\n",
    "            is_approx = False\n",
    "            has_format = False\n",
    "            for resp in resp_list:\n",
    "                if match_format.search(resp or \"\") is not None:\n",
    "                    has_format = True\n",
    "                guess = extract_final_number(resp or \"\")\n",
    "                truth = extract_final_number(a or \"\")\n",
    "                try:\n",
    "                    if truth is not None and guess is not None:\n",
    "                        g = float(guess)\n",
    "                        t = float(truth)\n",
    "                        if g == t:\n",
    "                            is_correct = True\n",
    "                        ratio = g / t if t != 0 else 0.0\n",
    "                        if 0.9 <= ratio <= 1.1:\n",
    "                            is_approx = True\n",
    "                except Exception:\n",
    "                    pass\n",
    "                if is_correct and is_approx and has_format:\n",
    "                    break\n",
    "\n",
    "            total += 1\n",
    "            if is_correct:\n",
    "                strict_correct += 1\n",
    "            if is_approx:\n",
    "                approx_correct += 1\n",
    "            if has_format:\n",
    "                format_ok += 1\n",
    "\n",
    "    acc = 100.0 * strict_correct / max(1, total)\n",
    "    approx_acc = 100.0 * approx_correct / max(1, total)\n",
    "    fmt_acc = 100.0 * format_ok / max(1, total)\n",
    "\n",
    "    print(f\"Total examples: {total}\")\n",
    "    print(f\"Strict accuracy: {acc:.2f}%\")\n",
    "    print(f\"Approx accuracy: {approx_acc:.2f}%\")\n",
    "    print(f\"Format accuracy: {fmt_acc:.2f}%\")\n",
    "    return dict(\n",
    "        total=total,\n",
    "        strict_accuracy=acc,\n",
    "        approx_accuracy=approx_acc,\n",
    "        format_accuracy=fmt_acc,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f140f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Baseline evaluation before GRPO ===\n",
    "\n",
    "baseline_sampler = build_sampler(lora_policy, tokenizer, model_config)\n",
    "print(\"Evaluating baseline policy on a small test subset...\")\n",
    "baseline_metrics = evaluate_dataset(test_dataset, baseline_sampler, num_passes=1)\n",
    "baseline_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5641b37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RLCluster, optimizer, and GRPOLearner setup ===\n",
    "\n",
    "ckpt_options = ocp.CheckpointManagerOptions(\n",
    "    save_interval_steps=SAVE_INTERVAL_STEPS,\n",
    "    max_to_keep=MAX_TO_KEEP,\n",
    ")\n",
    "\n",
    "log_dir = \"/kaggle/working/tensorboard/grpo\"\n",
    "metrics_opts = metrics_logger.MetricsLoggerOptions(\n",
    "    log_dir=log_dir,\n",
    "    flush_every_n_steps=20,\n",
    ")\n",
    "\n",
    "schedule = optax.schedules.warmup_cosine_decay_schedule(\n",
    "    init_value=0.0,\n",
    "    peak_value=LEARNING_RATE,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    decay_steps=MAX_STEPS,\n",
    "    end_value=0.0,\n",
    ")\n",
    "optimizer = optax.adamw(\n",
    "    learning_rate=schedule,\n",
    "    b1=B1,\n",
    "    b2=B2,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "if MAX_GRAD_NORM is not None:\n",
    "    optimizer = optax.chain(\n",
    "        optax.clip_by_global_norm(MAX_GRAD_NORM),\n",
    "        optimizer,\n",
    "    )\n",
    "\n",
    "cluster_config = rl_cluster_lib.ClusterConfig(\n",
    "    role_to_mesh={\n",
    "        rl_cluster_lib.Role.ACTOR: mesh,\n",
    "        rl_cluster_lib.Role.REFERENCE: mesh,\n",
    "        rl_cluster_lib.Role.ROLLOUT: mesh,\n",
    "    },\n",
    "    rollout_engine=\"vanilla\",\n",
    "    offload_to_cpu=False,\n",
    "    training_config=rl_cluster_lib.RLTrainingConfig(\n",
    "        actor_optimizer=optimizer,\n",
    "        eval_every_n_steps=64,\n",
    "        max_steps=MAX_STEPS,\n",
    "        mini_batch_size=TRAIN_MICRO_BATCH_SIZE,\n",
    "        train_micro_batch_size=TRAIN_MICRO_BATCH_SIZE,\n",
    "        metrics_logging_options=metrics_opts,\n",
    "        checkpoint_root_directory=CKPT_DIR,\n",
    "        checkpointing_options=ckpt_options,\n",
    "    ),\n",
    "    rollout_config=base_rollout.RolloutConfig(\n",
    "        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n",
    "        max_prompt_length=MAX_PROMPT_LENGTH,\n",
    "        kv_cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
    "        temperature=TEMPERATURE,\n",
    "        top_p=TOP_P,\n",
    "        top_k=TOP_K,\n",
    "        eos_tokens=EOS_TOKENS,\n",
    "    ),\n",
    ")\n",
    "\n",
    "grpo_config = GRPOConfig(\n",
    "    num_generations=NUM_GENERATIONS,\n",
    "    num_iterations=NUM_ITERATIONS,\n",
    "    beta=0.08,\n",
    "    epsilon=0.2,\n",
    ")\n",
    "\n",
    "rl_cluster = rl_cluster_lib.RLCluster(\n",
    "    actor=lora_policy,\n",
    "    reference=base_gemma,\n",
    "    tokenizer=tokenizer,\n",
    "    cluster_config=cluster_config,\n",
    ")\n",
    "\n",
    "grpo_trainer = GRPOLearner(\n",
    "    rl_cluster=rl_cluster,\n",
    "    reward_fns=[\n",
    "        reward_format_exact,\n",
    "        reward_format_soft,\n",
    "        reward_cast_math_and_completeness,\n",
    "    ],\n",
    "    algo_config=grpo_config,\n",
    ")\n",
    "\n",
    "print(\"RLCluster and GRPOLearner ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97c895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Run GRPO training ===\n",
    "\n",
    "with mesh:\n",
    "    show_hbm_usage()\n",
    "    grpo_trainer.train(train_dataset, val_dataset)\n",
    "\n",
    "print(\"GRPO training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e2018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load final trained LoRA params and re-evaluate ===\n",
    "\n",
    "trained_ckpt_path = os.path.join(\n",
    "    CKPT_DIR, \"actor\", str(MAX_STEPS), \"model_params\"\n",
    ")\n",
    "\n",
    "abs_lora = jax.tree.map(\n",
    "    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
    "    nnx.state(lora_policy, nnx.LoRAParam),\n",
    ")\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "trained_lora_params = checkpointer.restore(trained_ckpt_path, target=abs_lora)\n",
    "\n",
    "nnx.update(\n",
    "    lora_policy,\n",
    "    jax.tree.map(\n",
    "        lambda a, b: b,\n",
    "        nnx.state(lora_policy, nnx.LoRAParam),\n",
    "        trained_lora_params,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Loaded trained LoRA params into policy model.\")\n",
    "\n",
    "finetuned_sampler = build_sampler(lora_policy, tokenizer, model_config)\n",
    "print(\"Evaluating finetuned policy on test subset...\")\n",
    "finetuned_metrics = evaluate_dataset(test_dataset, finetuned_sampler, num_passes=1)\n",
    "finetuned_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954ea601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Export final Tunix checkpoint (no safetensors) ===\n",
    "# We package the final ACTOR checkpoint directory so judges can load it with Tunix.\n",
    "\n",
    "final_export_dir = \"./tunix_dsa_cast_grpo_actor_ckpt\"\n",
    "if os.path.exists(final_export_dir):\n",
    "    shutil.rmtree(final_export_dir)\n",
    "\n",
    "actor_step_dir = os.path.dirname(trained_ckpt_path)\n",
    "shutil.copytree(actor_step_dir, final_export_dir)\n",
    "\n",
    "print(\"Copied final actor checkpoint to:\", final_export_dir)\n",
    "print(\"\\nContents:\")\n",
    "for root, dirs, files in os.walk(final_export_dir):\n",
    "    level = root.replace(final_export_dir, \"\").count(os.sep)\n",
    "    indent = \" \" * (2 * level)\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = \" \" * (2 * (level + 1))\n",
    "    for f in files:\n",
    "        size_mb = os.path.getsize(os.path.join(root, f)) / (1024 * 1024)\n",
    "        print(f\"{subindent}{f}  ({size_mb:.2f} MB)\")\n",
    "\n",
    "shutil.make_archive(\"tunix_dsa_cast_grpo_actor_ckpt\", \"zip\", final_export_dir)\n",
    "print(\"\\nCreated zip archive: tunix_dsa_cast_grpo_actor_ckpt.zip\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
