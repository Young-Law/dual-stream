{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma 3 (1B‑IT) Dual‑Stream Training – **SFT → GRPO (DSA‑CAST, No‑LoRA)**\n",
    "\n",
    "This notebook glues together two workflows into a **single, end‑to‑end training pipeline** on Gemma 3‑1B‑IT:\n",
    "\n",
    "1. **Supervised Fine‑Tuning (SFT)** – teach the model to answer math questions in a **structured Dual‑Stream** format.\n",
    "2. **GRPO (Group Relative Policy Optimization)** – further **align** the model to that format and reward correctness and structure.\n",
    "\n",
    "Training is **full‑parameter** in both stages (no LoRA adapters).\n",
    "\n",
    "---\n",
    "\n",
    "## The DSA monologue structure\n",
    "\n",
    "Here, “DSA” is a **Dual‑Stream Architecture**-based answering pattern with an internal monologue that is explicitly structured into four named sections:\n",
    "\n",
    "Inside the `<reasoning>...</reasoning>` block, the model must always write:\n",
    "\n",
    "- **Plan** – high‑level steps it will take to solve the problem.  \n",
    "- **Reasoning** – detailed step‑by‑step execution.  \n",
    "- **Evidence** – citations, calculations, and explicit checks that support the reasoning.  \n",
    "- **Sanity_check** – a quick check that the final answer “makes sense” (magnitude, units, edge‑cases).\n",
    "\n",
    "Then, outside the monologue, the model must put the final result in a separate `<answer>...</answer>` block:\n",
    "\n",
    "```text\n",
    "<reasoning>\n",
    "Plan:\n",
    "  ...\n",
    "\n",
    "Reasoning:\n",
    "  ...\n",
    "\n",
    "Evidence:\n",
    "  ...\n",
    "\n",
    "Sanity_check:\n",
    "  ...\n",
    "</reasoning>\n",
    "<answer>\n",
    "  42\n",
    "</answer>\n",
    "```\n",
    "\n",
    "This gives you:\n",
    "\n",
    "- A **human‑readable monologue stream** for oversight and debugging.\n",
    "- A **machine‑readable answer stream** for automatic grading and downstream tools.\n",
    "\n",
    "For the conceptual motivation and design details, see the accompanying whitepaper:  \n",
    "[The Inner Monologue: A Dual‑Stream Architecture for Verifiable Inner Alignment](https://docs.google.com/document/d/1np-I9zEKArodlDhQzfydhloCXIVK9O72g3OJSuo_-Wk/edit?usp=sharing)\n",
    "\n",
    "---\n",
    "\n",
    "## How this notebook is organized\n",
    "\n",
    "1. **Part 1 – SFT (Structured Dual‑Stream Supervised Fine‑Tuning)**\n",
    "   - Load Gemma 3‑1B‑IT via Kaggle/Tunix (no HF token needed).\n",
    "   - Format GSM8K into the new DSA template:\n",
    "     - `<reasoning>` block with **Plan / Reasoning / Evidence / Sanity_check** sections.\n",
    "     - Separate `<answer>` block with only the final scalar.\n",
    "   - Train with SFT (no LoRA).\n",
    "   - Optionally do a quick post‑SFT generation sanity‑check.\n",
    "   - **Zip and clean up the SFT checkpoints** so you keep a single artifact.\n",
    "\n",
    "2. **Part 2 – GRPO (DSA‑CAST Reinforcement Learning)**\n",
    "   - Re‑build a GSM8K‑style dataset for RL rollouts using the same template.\n",
    "   - Define **DSA‑CAST rewards** that look at:\n",
    "     - Dual‑Stream tags,\n",
    "     - Plan/Reasoning/Evidence/Sanity_check structure,\n",
    "     - and math correctness/completeness.\n",
    "   - Run GRPO with Tunix’ `RLCluster` + `GRPOLearner` (no LoRA).\n",
    "   - Evaluate before/after GRPO on GSM8K.\n",
    "   - Export the **final GRPO actor checkpoint as a single zip** and clean up.\n",
    "\n",
    "By default, the hyperparameters are set for a **debug‑scale run** so you can validate wiring and behavior.  \n",
    "Once you’re satisfied, you can increase `MAX_STEPS` etc. for a longer training run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 — Supervised Fine‑Tuning (SFT): Teaching the DSA Monologue\n",
    "\n",
    "This section is the original **SFT notebook**, lightly edited:\n",
    "\n",
    "- It uses GSM8K to teach the model to respond with a structured monologue inside `<reasoning>...</reasoning>` containing:\n",
    "  - Plan\n",
    "  - Reasoning\n",
    "  - Evidence\n",
    "  - Sanity_check\n",
    "- It keeps a separate `<answer>...</answer>` block for the final scalar answer.\n",
    "- Hyperparameters are reduced so that training runs quickly.\n",
    "- At the end of Part 1, we zip the SFT checkpoints and clean up their directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T21:07:49.768197Z",
     "iopub.status.busy": "2025-12-14T21:07:49.767972Z",
     "iopub.status.idle": "2025-12-14T21:08:07.121655Z",
     "shell.execute_reply": "2025-12-14T21:08:07.120290Z",
     "shell.execute_reply.started": "2025-12-14T21:07:49.768180Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/jax/_src/cloud_tpu_init.py:93: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX version: 0.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before InitGoogle() is written to STDERR\n",
      "E0000 00:00:1765746479.208854      12 common_lib.cc:648] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n",
      "=== Source Location Trace: === \n",
      "learning/45eac/tfrc/runtime/common_lib.cc:238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 8\n",
      "Device kind: TPU v5 lite\n",
      "JAX backend: tpu\n",
      "\n",
      "Devices:\n",
      "  [0] TPU_0(process=0,(0,0,0,0))\n",
      "  [1] TPU_1(process=0,(1,0,0,0))\n",
      "  [2] TPU_2(process=0,(0,1,0,0))\n",
      "  [3] TPU_3(process=0,(1,1,0,0))\n",
      "  [4] TPU_4(process=0,(0,2,0,0))\n",
      "  [5] TPU_5(process=0,(1,2,0,0))\n",
      "  [6] TPU_6(process=0,(0,3,0,0))\n",
      "  [7] TPU_7(process=0,(1,3,0,0))\n",
      "============================================================\n",
      "\n",
      "✓ TPU backend confirmed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import types\n",
    "import numpy as np\n",
    "\n",
    "SMOKE_TEST = os.environ.get(\"SMOKE_TEST\", \"1\") == \"1\"\n",
    "\n",
    "if SMOKE_TEST:\n",
    "    class _DummyDevice(types.SimpleNamespace):\n",
    "        device_kind = \"CPU\"\n",
    "\n",
    "    class _DummyMesh(types.SimpleNamespace):\n",
    "        pass\n",
    "\n",
    "    class _DummyJax(types.SimpleNamespace):\n",
    "        def __init__(self):\n",
    "            super().__init__(\n",
    "                __version__=\"0.0.0\",\n",
    "                config=types.SimpleNamespace(update=lambda *args, **kwargs: None),\n",
    "            )\n",
    "            self.numpy = np\n",
    "\n",
    "        def devices(self):\n",
    "            return [_DummyDevice()]\n",
    "\n",
    "        def default_backend(self):\n",
    "            return \"cpu\"\n",
    "\n",
    "        def make_mesh(self, shape, axis_names):\n",
    "            return _DummyMesh(shape=shape, axis_names=axis_names)\n",
    "\n",
    "    jax = _DummyJax()\n",
    "    jnp = np\n",
    "    print(\"Smoke-test mode enabled (set SMOKE_TEST=0 to run full pipeline on Kaggle TPU).\")\n",
    "else:\n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "    print(f\"JAX version: {jax.__version__}\")\n",
    "    print(f\"Device count: {len(jax.devices())}\")\n",
    "    print(f\"Device kind: {jax.devices()[0].device_kind}\")\n",
    "    print(f\"Backend: {jax.default_backend()}\")\n",
    "    if jax.default_backend() != 'tpu':\n",
    "        print(\"WARNING: Not running on TPU — select TPU in Kaggle for full training.\")\n",
    "    os.environ['XLA_FLAGS'] = (\n",
    "        '--xla_gpu_enable_triton_softmax_fusion=true '\n",
    "        '--xla_gpu_triton_gemm_any=True '\n",
    "        '--xla_gpu_enable_async_collectives=true'\n",
    "    )\n",
    "    os.environ['JAX_COMPILATION_CACHE_DIR'] = '/tmp/jax_cache'\n",
    "    os.environ['LIBTPU_INIT_ARGS'] = '--xla_enable_async_all_gather=true'\n",
    "    jax.config.update('jax_enable_x64', False)\n",
    "    jax.config.update('jax_default_matmul_precision', 'high')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T21:08:07.122575Z",
     "iopub.status.busy": "2025-12-14T21:08:07.122286Z",
     "iopub.status.idle": "2025-12-14T21:08:07.127517Z",
     "shell.execute_reply": "2025-12-14T21:08:07.126572Z",
     "shell.execute_reply.started": "2025-12-14T21:08:07.122542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Batch Size: 64\n",
      "Total Training Steps: 50\n",
      "✓ Configuration loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "KAGGLE_MODEL_HANDLE = \"google/gemma-3/transformers/gemma-3-1b-it\"\n",
    "\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "MESH_SHAPE = (1, 4)\n",
    "TRAIN_MICRO_BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 1  # DEBUG: 1 epoch for quick sanity check\n",
    "MAX_STEPS = 8  # tiny smoke-test default; raise for real runs\n",
    "WARMUP_STEPS = int(0.1 * MAX_STEPS)\n",
    "ADAM_BETA1 = 0.9\n",
    "ADAM_BETA2 = 0.999\n",
    "ADAM_EPSILON = 1e-8\n",
    "WEIGHT_DECAY = 0.01\n",
    "MAX_GRAD_NORM = 1.0\n",
    "CHECKPOINT_DIR = \"/kaggle/working/outputs_sft_full/checkpoints\"\n",
    "TENSORBOARD_DIR = \"/kaggle/working/outputs_sft_full/tensorboard\"\n",
    "SAVE_INTERVAL_STEPS = 2\n",
    "EVAL_INTERVAL_STEPS = 2\n",
    "LOG_INTERVAL_STEPS = 1\n",
    "\n",
    "print(f\"Global Batch Size: {TRAIN_MICRO_BATCH_SIZE * 8 * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Total Training Steps: {MAX_STEPS}\")\n",
    "print(\"✓ Configuration loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T21:08:07.128490Z",
     "iopub.status.busy": "2025-12-14T21:08:07.128319Z",
     "iopub.status.idle": "2025-12-14T21:08:24.630014Z",
     "shell.execute_reply": "2025-12-14T21:08:24.628854Z",
     "shell.execute_reply.started": "2025-12-14T21:08:07.128474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model handle: google/gemma-3/transformers/gemma-3-1b-it\n",
      "✓ Model downloaded to: /kaggle/input/gemma-3/transformers/gemma-3-1b-it/1\n",
      "\n",
      "Creating TPU mesh with shape (1, 4)...\n",
      "✓ TPU Mesh created successfully\n",
      "  Mesh shape: OrderedDict({'fsdp': 1, 'tp': 4})\n",
      "  Mesh axis names: ('fsdp', 'tp')\n"
     ]
    }
   ],
   "source": [
    "if SMOKE_TEST:\n",
    "    print(\"Smoke test: skipping KaggleHub download and model init.\")\n",
    "    class DummyTokenizer:\n",
    "        def encode(self, text):\n",
    "            return [i % 100 for i, _ in enumerate(text[:MAX_SEQ_LENGTH])]\n",
    "        def pad_id(self):\n",
    "            return 0\n",
    "        def eos_id(self):\n",
    "            return 1\n",
    "    tokenizer = DummyTokenizer()\n",
    "    mesh = jax.make_mesh(MESH_SHAPE, ('fsdp', 'tp'))\n",
    "    local_model_path = \"/kaggle/working/mock_model\"\n",
    "else:\n",
    "    import kagglehub\n",
    "    from tunix.models.gemma3 import model as gemma_lib\n",
    "    from tunix.models.gemma3 import params_safetensors as params_safetensors_lib\n",
    "    from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
    "    print(f\"Model handle: {KAGGLE_MODEL_HANDLE}\")\n",
    "    local_model_path = kagglehub.model_download(KAGGLE_MODEL_HANDLE)\n",
    "    print(f\"✓ Model downloaded to: {local_model_path}\")\n",
    "    print(f\"Creating TPU mesh with shape {MESH_SHAPE}...\")\n",
    "    mesh = jax.make_mesh(MESH_SHAPE, ('fsdp', 'tp'))\n",
    "    print(f\"✓ TPU Mesh created successfully\")\n",
    "    print(f\"  Mesh shape: {mesh.shape}\")\n",
    "    print(f\"  Mesh axis names: {mesh.axis_names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T21:08:24.630843Z",
     "iopub.status.busy": "2025-12-14T21:08:24.630563Z",
     "iopub.status.idle": "2025-12-14T21:08:50.741284Z",
     "shell.execute_reply": "2025-12-14T21:08:50.739907Z",
     "shell.execute_reply.started": "2025-12-14T21:08:24.630825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded successfully\n",
      "✓ Tokenizer loaded successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if SMOKE_TEST:\n",
    "    class DummyModel:\n",
    "        def get_model_input(self):\n",
    "            return {}\n",
    "    gemma3_model = DummyModel()\n",
    "    model_config = None\n",
    "    print(\"Smoke test: model creation skipped.\")\n",
    "else:\n",
    "    model_config = gemma_lib.ModelConfig.gemma3_1b()\n",
    "    gemma3_model = params_safetensors_lib.create_model_from_safe_tensors(\n",
    "        local_model_path,\n",
    "        model_config,\n",
    "        mesh,\n",
    "    )\n",
    "    print(\"✓ Model loaded successfully\")\n",
    "    tokenizer = tokenizer_lib.Tokenizer(tokenizer_path=f\"{local_model_path}/tokenizer.model\")\n",
    "    print(\"✓ Tokenizer loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T21:08:50.742188Z",
     "iopub.status.busy": "2025-12-14T21:08:50.742010Z",
     "iopub.status.idle": "2025-12-14T21:08:51.533770Z",
     "shell.execute_reply": "2025-12-14T21:08:51.532440Z",
     "shell.execute_reply.started": "2025-12-14T21:08:50.742170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sharding model across TPU devices...\n",
      "\n",
      "✓ Model ready for full fine-tuning\n",
      "Total parameters: 999,885,952\n",
      "Trainable parameters: 999,885,952\n",
      "Number of parameters: 314\n",
      "Sample param shape: (262144, 1152)\n",
      "Sample param dtype: bfloat16\n",
      "Sample param devices: [TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)]\n",
      "Device kind: TPU v5 lite\n",
      "✓✓✓ SUCCESS: Model parameters are on TPU!\n",
      "✓✓✓ Confirmed: TPU v5 lite detected\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "if SMOKE_TEST:\n",
    "    print(\"Smoke test: skipping sharding and optimizer state init.\")\n",
    "    pspecs = None\n",
    "else:\n",
    "    import flax.nnx as nnx\n",
    "    model_input = gemma3_model.get_model_input()\n",
    "    print(\"Sharding model across TPU devices...\")\n",
    "    with mesh:\n",
    "        state = nnx.state(gemma3_model)\n",
    "        pspecs = nnx.get_partition_spec(state)\n",
    "    print(\"✓ Model sharded across mesh\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T21:10:09.895820Z",
     "iopub.status.busy": "2025-12-14T21:10:09.895490Z",
     "iopub.status.idle": "2025-12-14T21:10:11.147818Z",
     "shell.execute_reply": "2025-12-14T21:10:11.146675Z",
     "shell.execute_reply.started": "2025-12-14T21:10:09.895801Z"
    }
   },
   "outputs": [],
   "source": [
    "# Legacy helper removed; see updated formatter below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dataset loader for DSA competition JSONL.\"\"\"\n",
    "import json\n",
    "import random\n",
    "import glob\n",
    "from pathlib import Path\n",
    "SYSTEM_PROMPT = globals().get(\"SYSTEM_PROMPT\", \"Respond with <reasoning> and <answer> blocks.\")\n",
    "\n",
    "DSA_JSONL_FILENAME = \"dsa_competition_sft_dataset_v1_reasoning_answer.jsonl\"\n",
    "DSA_JSONL_CANDIDATES = [\n",
    "    Path(DSA_JSONL_FILENAME),\n",
    "    Path(\"/kaggle/working\") / DSA_JSONL_FILENAME,\n",
    "]\n",
    "DSA_JSONL_CANDIDATES += [Path(p) for p in glob.glob(f\"/kaggle/input/**/{DSA_JSONL_FILENAME}\", recursive=True)]\n",
    "DSA_JSONL_PATH = next((p for p in DSA_JSONL_CANDIDATES if p.exists()), None)\n",
    "\n",
    "if DSA_JSONL_PATH is None:\n",
    "    DSA_JSONL_PATH = Path(\"/kaggle/working\") / DSA_JSONL_FILENAME\n",
    "    DSA_JSONL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Dataset not found; writing tiny smoke-test sample to {DSA_JSONL_PATH}.\")\n",
    "    sample_rows = [\n",
    "        {\"question\": \"What is 2 + 3?\", \"reasoning\": '<reasoning>Plan:\\n- Add the two numbers.\\nReasoning:\\n- 2 + 3 = 5.\\nEvidence:\\n- Direct addition.\\nSanity_check:\\n- 5 is reasonable.\\n</reasoning>', \"answer\": \"<answer>5</answer>\"},\n",
    "        {\"question\": \"If you have 10 apples and eat 4, how many are left?\", \"reasoning\": '<reasoning>Plan:\\n- Subtract eaten apples.\\nReasoning:\\n- 10 - 4 = 6.\\nEvidence:\\n- Basic subtraction.\\nSanity_check:\\n- Result is positive and less than start.\\n</reasoning>', \"answer\": \"<answer>6</answer>\"},\n",
    "    ]\n",
    "    with DSA_JSONL_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for row in sample_rows:\n",
    "            json.dump(row, f)\n",
    "            f.write(\"\\n\")\n",
    "else:\n",
    "    print(f\"Found dataset at {DSA_JSONL_PATH}\")\n",
    "\n",
    "reasoning_start = \"<reasoning>\"\n",
    "reasoning_end = \"</reasoning>\"\n",
    "solution_start = \"<answer>\"\n",
    "solution_end = \"</answer>\"\n",
    "\n",
    "def _ensure_block(text: str, start: str, end: str) -> str:\n",
    "    text = text or \"\"\n",
    "    if start in text and end in text:\n",
    "        return text\n",
    "    return f\"{start}\\n{text.strip()}\\n{end}\"\n",
    "\n",
    "def format_chat_record_for_gemma(rec):\n",
    "    question = rec.get(\"question\") or rec.get(\"prompt\") or rec.get(\"input\") or \"Solve the problem.\"\n",
    "    reasoning_block = rec.get(\"reasoning\") or rec.get(\"solution\") or rec.get(\"text\") or \"\"\n",
    "    answer_block = rec.get(\"answer\") or rec.get(\"final_answer\") or \"\"\n",
    "    reasoning_block = _ensure_block(reasoning_block, reasoning_start, reasoning_end)\n",
    "    answer_block = _ensure_block(answer_block, solution_start, solution_end)\n",
    "    text = rec.get(\"text\")\n",
    "    if not text:\n",
    "        text = (\n",
    "            f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\\n{question}<end_of_turn>\\n\"\n",
    "            f\"<start_of_turn>model\\n{reasoning_block}\\n{answer_block}\\n<end_of_turn>\"\n",
    "        )\n",
    "    return {\"text\": text}\n",
    "\n",
    "raw_records = []\n",
    "with DSA_JSONL_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        raw_records.append(json.loads(line))\n",
    "\n",
    "random.shuffle(raw_records)\n",
    "if not raw_records:\n",
    "    raise ValueError(\"Dataset is empty even after smoke-test fallback.\")\n",
    "\n",
    "split = max(1, int(0.1 * len(raw_records))) if len(raw_records) > 1 else 1\n",
    "formatted_test = [format_chat_record_for_gemma(r) for r in raw_records[:split]]\n",
    "formatted_train = [format_chat_record_for_gemma(r) for r in raw_records[split:]]\n",
    "if not formatted_train:\n",
    "    formatted_train = formatted_test\n",
    "print(f\"Prepared {len(formatted_train)} train and {len(formatted_test)} eval examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T21:11:50.832876Z",
     "iopub.status.busy": "2025-12-14T21:11:50.832531Z",
     "iopub.status.idle": "2025-12-14T21:11:50.843194Z",
     "shell.execute_reply": "2025-12-14T21:11:50.842412Z",
     "shell.execute_reply.started": "2025-12-14T21:11:50.832848Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (480592263.py, line 26)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m{user_msg}<end_of_turn>\u001b[39m\n                           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"-\" * 60)\n",
    "example = formatted_train[0][\"text\"] if formatted_train else \"(no data)\"\n",
    "print(example)\n",
    "print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T21:11:51.061871Z",
     "iopub.status.busy": "2025-12-14T21:11:51.061664Z",
     "iopub.status.idle": "2025-12-14T21:11:51.085216Z",
     "shell.execute_reply": "2025-12-14T21:11:51.084307Z",
     "shell.execute_reply.started": "2025-12-14T21:11:51.061844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'formatted_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mformatted_train\u001b[49m[\u001b[32m100\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'formatted_train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"-\" * 60)\n",
    "example = formatted_train[0][\"text\"] if formatted_train else \"(no data)\"\n",
    "print(example)\n",
    "print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T21:11:51.097381Z",
     "iopub.status.busy": "2025-12-14T21:11:51.097188Z",
     "iopub.status.idle": "2025-12-14T21:11:51.429188Z",
     "shell.execute_reply": "2025-12-14T21:11:51.428011Z",
     "shell.execute_reply.started": "2025-12-14T21:11:51.097364Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'formatted_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m TrainingInput(input_tokens=input_tokens, input_mask=loss_mask)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Create Grain datasets\u001b[39;00m\n\u001b[32m     38\u001b[39m train_grain = (\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     grain.MapDataset.source(\u001b[43mformatted_train\u001b[49m)\n\u001b[32m     40\u001b[39m     .map(tokenize_function)\n\u001b[32m     41\u001b[39m     .shuffle(seed=\u001b[32m42\u001b[39m)\n\u001b[32m     42\u001b[39m     .repeat(NUM_EPOCHS)\n\u001b[32m     43\u001b[39m     .batch(batch_size=TRAIN_MICRO_BATCH_SIZE, drop_remainder=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     44\u001b[39m )\n\u001b[32m     46\u001b[39m eval_grain = (\n\u001b[32m     47\u001b[39m     grain.MapDataset.source(formatted_test)\n\u001b[32m     48\u001b[39m     .map(tokenize_function)\n\u001b[32m     49\u001b[39m     .batch(batch_size=TRAIN_MICRO_BATCH_SIZE, drop_remainder=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     50\u001b[39m )\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Train batches: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_grain)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'formatted_train' is not defined"
     ]
    }
   ],
   "source": [
    "if SMOKE_TEST:\n",
    "    import numpy as np\n",
    "    print(\"Smoke test: using lightweight numpy tokenization.\")\n",
    "\n",
    "    def tokenize_function(example):\n",
    "        tokens = tokenizer.encode(example[\"text\"])\n",
    "        tokens = tokens[:MAX_SEQ_LENGTH]\n",
    "        pad_id = tokenizer.pad_id() if hasattr(tokenizer, \"pad_id\") else 0\n",
    "        if len(tokens) < MAX_SEQ_LENGTH:\n",
    "            tokens = tokens + [pad_id] * (MAX_SEQ_LENGTH - len(tokens))\n",
    "        mask = [1.0 if idx < len(example[\"text\"]) else 0.0 for idx in range(MAX_SEQ_LENGTH)]\n",
    "        return {\"input_tokens\": np.array(tokens, dtype=np.int32), \"input_mask\": np.array(mask, dtype=np.float32)}\n",
    "\n",
    "    train_grain = [tokenize_function(ex) for ex in formatted_train]\n",
    "    eval_grain = [tokenize_function(ex) for ex in formatted_test]\n",
    "    print(f\"✓ (smoke) Train batches: {len(train_grain)}\")\n",
    "    print(f\"✓ (smoke) Eval batches: {len(eval_grain)}\")\n",
    "else:\n",
    "    import grain.python as grain\n",
    "    import numpy as np\n",
    "    from tunix.sft import metrics_logger as tmetrics\n",
    "    from tunix.sft.peft_trainer import TrainingInput\n",
    "    tmetrics.wandb = None\n",
    "\n",
    "    def tokenize_function(example):\n",
    "        full_text = example[\"text\"]\n",
    "        full_tokens = tokenizer.encode(full_text)\n",
    "        prompt_text = full_text.split(\"<start_of_turn>model\")[0] + \"<start_of_turn>model\\n\"\n",
    "        prompt_tokens = tokenizer.encode(prompt_text)\n",
    "        prompt_len = len(prompt_tokens)\n",
    "        if len(full_tokens) > MAX_SEQ_LENGTH:\n",
    "            full_tokens = full_tokens[:MAX_SEQ_LENGTH]\n",
    "        else:\n",
    "            pad_token = tokenizer.pad_id() if hasattr(tokenizer, \"pad_id\") else tokenizer.eos_id()\n",
    "            full_tokens = full_tokens + [pad_token] * (MAX_SEQ_LENGTH - len(full_tokens))\n",
    "        input_tokens = np.array(full_tokens, dtype=np.int32)\n",
    "        loss_mask = np.zeros_like(input_tokens, dtype=np.float32)\n",
    "        seq_len = min(len(tokenizer.encode(full_text)), MAX_SEQ_LENGTH)\n",
    "        if seq_len > prompt_len:\n",
    "            loss_mask[prompt_len:seq_len] = 1.0\n",
    "        return TrainingInput(input_tokens=input_tokens, input_mask=loss_mask)\n",
    "\n",
    "    train_grain = (\n",
    "        grain.MapDataset.source(formatted_train)\n",
    "        .map(tokenize_function)\n",
    "        .shuffle(seed=42)\n",
    "        .repeat(NUM_EPOCHS)\n",
    "        .batch(batch_size=TRAIN_MICRO_BATCH_SIZE, drop_remainder=True)\n",
    "    )\n",
    "    eval_grain = (\n",
    "        grain.MapDataset.source(formatted_test)\n",
    "        .map(tokenize_function)\n",
    "        .batch(batch_size=TRAIN_MICRO_BATCH_SIZE, drop_remainder=True)\n",
    "    )\n",
    "    print(f\"✓ Train batches: {len(train_grain):,}\")\n",
    "    print(f\"✓ Eval batches: {len(eval_grain):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-14T21:11:51.429882Z",
     "iopub.status.idle": "2025-12-14T21:11:51.430231Z",
     "shell.execute_reply": "2025-12-14T21:11:51.429996Z",
     "shell.execute_reply.started": "2025-12-14T21:11:51.429986Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "if SMOKE_TEST:\n",
    "    print(\"Smoke test: skipping optimizer setup (optax).\"); schedule = optimizer = None\n",
    "else:\n",
    "    import optax\n",
    "    effective_warmup_steps = min(WARMUP_STEPS, max(1, MAX_STEPS - 1))\n",
    "    schedule = optax.warmup_cosine_decay_schedule(\n",
    "        init_value=0.0,\n",
    "        peak_value=LEARNING_RATE,\n",
    "        warmup_steps=effective_warmup_steps,\n",
    "        decay_steps=MAX_STEPS - WARMUP_STEPS,\n",
    "        end_value=LEARNING_RATE * 0.1,\n",
    "    )\n",
    "    optimizer = optax.chain(\n",
    "        optax.clip_by_global_norm(MAX_GRAD_NORM),\n",
    "        optax.scale_by_adam(b1=ADAM_BETA1, b2=ADAM_BETA2, eps=ADAM_EPSILON),\n",
    "        optax.add_decayed_weights(WEIGHT_DECAY),\n",
    "        optax.scale_by_schedule(schedule),\n",
    "        optax.scale(-1.0),\n",
    "    )\n",
    "    print(\"✓ Optimizer configured:\")\n",
    "    print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "    print(f\"  Warmup steps: {WARMUP_STEPS}\")\n",
    "    print(f\"  Total steps: {MAX_STEPS}\")\n",
    "    print(f\"  Weight decay: {WEIGHT_DECAY}\")\n",
    "    print(f\"  Max grad norm: {MAX_GRAD_NORM}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-14T21:11:51.430538Z",
     "iopub.status.idle": "2025-12-14T21:11:51.431107Z",
     "shell.execute_reply": "2025-12-14T21:11:51.430649Z",
     "shell.execute_reply.started": "2025-12-14T21:11:51.430641Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "if SMOKE_TEST:\n",
    "    print(\"Smoke test: skipping trainer wiring; define placeholders only.\")\n",
    "    PeftTrainer = TrainingConfig = MetricsLoggerOptions = None\n",
    "else:\n",
    "    from tunix import PeftTrainer, TrainingConfig, MetricsLoggerOptions\n",
    "    import orbax.checkpoint as ocp\n",
    "    from tunix.sft import metrics_logger as tmetrics\n",
    "    tmetrics.wandb = None\n",
    "    checkpointing_options = ocp.CheckpointManagerOptions(\n",
    "        save_interval_steps=SAVE_INTERVAL_STEPS,\n",
    "        max_to_keep=3,\n",
    "    )\n",
    "    training_config = TrainingConfig(\n",
    "        max_steps=MAX_STEPS,\n",
    "        eval_every_n_steps=EVAL_INTERVAL_STEPS,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        checkpoint_root_directory=CHECKPOINT_DIR,\n",
    "        checkpointing_options=checkpointing_options,\n",
    "        metrics_logging_options=None,\n",
    "    )\n",
    "    print(\"✓ Training configuration created\")\n",
    "    print(f\"  Max steps: {MAX_STEPS}\")\n",
    "    print(f\"  Micro batch size: {TRAIN_MICRO_BATCH_SIZE}\")\n",
    "    print(f\"  Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "    print(f\"  Effective batch size: {TRAIN_MICRO_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "    print(f\"  Eval interval: {EVAL_INTERVAL_STEPS}\")\n",
    "    print(f\"  Save interval: {SAVE_INTERVAL_STEPS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "if SMOKE_TEST:\n",
    "    print(\"Smoke test: skipping training loop and evaluation.\")\n",
    "else:\n",
    "    from tunix.sft import utils\n",
    "    def gen_model_input_fn(training_input):\n",
    "        return utils.make_model_input(training_input.input_tokens, training_input.input_mask)\n",
    "    trainer = PeftTrainer(\n",
    "        training_config,\n",
    "        model_input_fn=gen_model_input_fn,\n",
    "        optimizer=optimizer,\n",
    "        param_spec=pspecs,\n",
    "        mesh=mesh,\n",
    "        model_factory=lambda: gemma3_model,\n",
    "        metrics_logger_options=MetricsLoggerOptions(metrics_to_log=(\"loss\",)),\n",
    "    )\n",
    "    print(\"Starting SFT training...\")\n",
    "    trainer.train(train_grain, eval_input=eval_grain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T21:11:51.461408Z",
     "iopub.status.busy": "2025-12-14T21:11:51.461145Z",
     "iopub.status.idle": "2025-12-14T21:11:51.499223Z",
     "shell.execute_reply": "2025-12-14T21:11:51.498255Z",
     "shell.execute_reply.started": "2025-12-14T21:11:51.461371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Starting Full Fine-Tuning on TPU v5e-8\n",
      "============================================================\n",
      "Max steps: 1500\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'formatted_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMax steps: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_STEPS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining examples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43mformatted_train\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEval examples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(formatted_test)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBatch size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTRAIN_MICRO_BATCH_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'formatted_train' is not defined"
     ]
    }
   ],
   "source": [
    "if SMOKE_TEST:\n",
    "    print('Smoke test: skipping cell 15.')\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T21:12:00.278035Z",
     "iopub.status.busy": "2025-12-14T21:12:00.277692Z",
     "iopub.status.idle": "2025-12-14T21:12:00.312298Z",
     "shell.execute_reply": "2025-12-14T21:12:00.311267Z",
     "shell.execute_reply.started": "2025-12-14T21:12:00.278012Z"
    }
   },
   "outputs": [],
   "source": [
    "if SMOKE_TEST:\n",
    "    print('Smoke test: skipping cell 16.')\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T21:12:01.114042Z",
     "iopub.status.busy": "2025-12-14T21:12:01.113609Z",
     "iopub.status.idle": "2025-12-14T21:13:19.526827Z",
     "shell.execute_reply": "2025-12-14T21:13:19.525598Z",
     "shell.execute_reply.started": "2025-12-14T21:12:01.114010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Testing Trained Model (Strict Format)\n",
      "============================================================\n",
      "\n",
      "[Test 1] Question: What is the square root of 144?\n",
      "------------------------------------------------------------\n",
      "Response:\n",
      "reason:\n",
      "The square root of 144 is 12.\n",
      "\n",
      "</reasoning>\n",
      "<answer>\n",
      "12\n",
      "</answer>\n",
      "============================================================\n",
      "\n",
      "[Test 2] Question: If a shirt costs $25 and is on sale for 20% off, what is the sale price?\n",
      "------------------------------------------------------------\n",
      "Response:\n",
      "reason:\n",
      "The sale price is 25*.20 = $(25*.20=5)5 off. So the sale price is 25-5 = $(25-5=20)20.\n",
      "\n",
      "<answer>\n",
      "20\n",
      "</answer>\n",
      "============================================================\n",
      "\n",
      "[Test 3] Question: A train travels 60 miles in 45 minutes. What is its speed in miles per hour?\n",
      "------------------------------------------------------------\n",
      "Response:\n",
      "Plan:\n",
      "- We will break the problem into smaller steps and solve them one by one.\n",
      "Reasoning:\n",
      "First find the total number of minutes in 45 minutes: 45 minutes * 60 minutes/hour = (45*60=2700)2700 minutes Then divide the total number of minutes by the number of minutes per hour to find the total number of hours: 2700 minutes / 60 minutes/hour = (2700/60=45)45 hours Then divide the total number of hours by the number of hours per day to find the total number of days: 45 hours / 2 = (45/2=22.5)22.5 days Then divide the total number of days by the number of days per year to find the total number of years: 22.5 days / 365 days/year = (22.5/365=0.62)0.62 years Then divide the total number of years by the number of years per year to find the total number of years: 0.62 years * 365 days/year = (0.62*365=220.9)220.9 years Then divide the total number of years by the number of years per year to find the total number of years in a year: 220.9 years / 365 days/year = (220.9/365=0.62)0.62 years Then divide the total number of years in a year by the number of years in a year to find the total number of years in a year: 0.62 years * 365 days/year = (0.62*365=220.9)220.9 years Then divide the total number of years in a year by the number of years in a year to find the total number of years in a year: 220.9 years / 365 days/year = (220.9/365=0.62)0.62 years Then divide the total number of years in a year by the number of years in a year to find the total number of years in a year: 0.62 years * 36\n",
      "============================================================\n",
      "\n",
      "[Test 4] Question: What is 15% of 200?\n",
      "------------------------------------------------------------\n",
      "Response:\n",
      "reason:\n",
      "15/100 * 200 = (15/100*200=30)30\n",
      "\n",
      "<answer>\n",
      "30</answer>\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "if SMOKE_TEST:\n",
    "    print('Smoke test: skipping cell 17.')\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T21:13:19.527854Z",
     "iopub.status.busy": "2025-12-14T21:13:19.527669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluating with Majority Voting (k=1)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac8e72fd234b447b9cf6396794910ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e0365be8ed4c8bbf10f2cdff990b0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "main/train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9095dc40300e494984e8fd821c9f57e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "main/test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7223b538339a4bec86468198737cf6a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c8961a2dd4469b99b2ae12d5a77809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a27053e51d458fb0de4d01853724e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Voting:   0%|          | 0/1319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if SMOKE_TEST:\n",
    "    print('Smoke test: skipping cell 18.')\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export SFT checkpoints as a zip & clean up\n",
    "\n",
    "The SFT trainer writes a full Tunix checkpoint tree under `CHECKPOINT_DIR` and TensorBoard\n",
    "logs under `TENSORBOARD_DIR`. To keep the number of files small and make it easy to download\n",
    "the weights, we:\n",
    "\n",
    "1. Zip **only** the SFT checkpoint tree into a single archive.\n",
    "2. Remove the original checkpoint and TensorBoard directories (they can always be recreated by re‑running SFT).\n",
    "\n",
    "> **Note** – This step assumes that SFT training has already run and produced at least one checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SMOKE_TEST:\n",
    "    print('Smoke test: skipping cell 20.')\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 — GRPO with DSA‑CAST Rewards (Reinforcement Learning)\n",
    "\n",
    "This section is your original **DSA‑CAST + Tunix GRPO notebook**, embedded after SFT.\n",
    "\n",
    "At a high level, it does:\n",
    "\n",
    "1. **Environment & data setup**\n",
    "   - Logs in to Hugging Face (via Kaggle secret).\n",
    "   - Ensures JAX + Tunix are installed on the TPU.\n",
    "   - Loads GSM8K from TFDS or a Kaggle dataset into a rollout‑friendly format:\n",
    "     - each example has a `prompts` field already formatted with the Dual‑Stream template\n",
    "     - plus `question` and `answer` fields used by the reward functions.\n",
    "\n",
    "2. **Reward design (DSA‑CAST)**\n",
    "   - `reward_format_exact`: strict regex check for the full `<reasoning>...<answer>...` layout.\n",
    "   - `reward_format_soft`: softer “tag hygiene” score that penalizes missing or repeated tags.\n",
    "   - `reward_cast_math_and_completeness`: CAST‑style scoring of:\n",
    "     - math accuracy,\n",
    "     - solution completeness,\n",
    "     - plus an extra format bonus.\n",
    "\n",
    "3. **GRPO training loop**\n",
    "   - Builds a Tunix `RLCluster` with:\n",
    "     - an **actor model** (the policy we update) and\n",
    "     - a **reference model** (kept frozen).\n",
    "   - Uses `GRPOLearner` to:\n",
    "     1. Sample `NUM_GENERATIONS` rollouts per prompt.\n",
    "     2. Score those rollouts with the DSA‑CAST reward.\n",
    "     3. Apply GRPO updates to the actor, keeping the reference fixed.\n",
    "\n",
    "4. **Baseline & post‑GRPO evaluation**\n",
    "   - Evaluate the base Gemma 3 1B‑IT model (pre‑GRPO) on GSM8K.\n",
    "   - Evaluate the GRPO‑trained actor on the same test data.\n",
    "   - Compare accuracy, “partial credit”, and format‑adherence metrics.\n",
    "\n",
    "5. **Export & cleanup**\n",
    "   - Zip the **best actor checkpoint** into a single file:\n",
    "     - `tunix_dsa_cast_grpo_actor_ckpt.zip`\n",
    "   - Remove the GRPO checkpoint tree to keep Kaggle’s output under its file limits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSA-CAST + Tunix GRPO on Gemma3-1B (TPU, Kaggle)\n",
    "\n",
    "This notebook:\n",
    "\n",
    "1. Sets up **Gemma3-1B-IT** on a Kaggle TPU using **Tunix**.\n",
    "2. Uses the `<reasoning> ... </reasoning>` and `<answer> ... </answer>` format for math problems (GSM8K-style).\n",
    "3. Defines a **CAST-style reward** that strongly favors:\n",
    "   - mathematical accuracy, and  \n",
    "   - answer completeness & proper tagging.\n",
    "4. Runs a **Tunix GRPO** reinforcement learning loop using that reward.\n",
    "5. Saves the final **Tunix checkpoint (no safetensors export)** so it can be re-used in another notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SMOKE_TEST:\n",
    "    print('Smoke test: skipping cell 23.')\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SMOKE_TEST:\n",
    "    print('Smoke test: skipping cell 24.')\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SMOKE_TEST:\n",
    "    print('Smoke test: skipping cell 25.')\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SMOKE_TEST:\n",
    "    print('Smoke test: skipping cell 26.')\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SMOKE_TEST:\n",
    "    print('Smoke test: skipping cell 27.')\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SMOKE_TEST:\n",
    "    print('Smoke test: skipping cell 28.')\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SMOKE_TEST:\n",
    "    print('Smoke test: skipping cell 29.')\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SMOKE_TEST:\n",
    "    print('Smoke test: skipping cell 30.')\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SMOKE_TEST:\n",
    "    print('Smoke test: skipping cell 31.')\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DSA‑CAST Reward Functions (What the RL Signal Is Measuring)\n",
    "\n",
    "The next cell defines three core reward functions used by GRPO, all of which\n",
    "are aware of the **Plan / Reasoning / Evidence / Sanity_check** structure\n",
    "inside `<reasoning>...</reasoning>` as well as the outer `<answer>...</answer>` block.\n",
    "\n",
    "1. **`reward_format_exact`**  \n",
    "   - Uses a strict regular expression over the full completion.  \n",
    "   - Gives a high reward when the output looks like:\n",
    "\n",
    "     ```text\n",
    "     <reasoning>\n",
    "     Plan:\n",
    "       ...\n",
    "\n",
    "     Reasoning:\n",
    "       ...\n",
    "\n",
    "     Evidence:\n",
    "       ...\n",
    "\n",
    "     Sanity_check:\n",
    "       ...\n",
    "     </reasoning>\n",
    "     <answer>\n",
    "       ...single final scalar...\n",
    "     </answer>\n",
    "     ```\n",
    "\n",
    "   - Any major deviation (missing tags, missing headings, wrong order, multiple answer blocks, etc.) receives 0.\n",
    "\n",
    "2. **`reward_format_soft`**  \n",
    "   - Provides a smoother shaping signal when the model is “on the way” to the desired format.  \n",
    "   - It:\n",
    "     - rewards the presence of `<reasoning>...</reasoning>` and `<answer>...</answer>` tags,\n",
    "     - rewards each of the four headings when present,\n",
    "     - adds extra reward when the headings appear in the correct order,\n",
    "     - and penalizes missing or badly ordered structure.\n",
    "\n",
    "3. **`reward_cast_math_and_completeness`**  \n",
    "   - Calls `cast_style_scores`, which:\n",
    "     - extracts the numeric answer from the `<answer> ... </answer>` block,\n",
    "     - compares it to the GSM8K ground‑truth answer (with some tolerance),\n",
    "     - and scores structural completeness based on:\n",
    "       - presence and order of Plan / Reasoning / Evidence / Sanity_check,\n",
    "       - and non‑trivial reasoning content inside `<reasoning>...</reasoning>`.\n",
    "   - Then combines:\n",
    "     - **math accuracy** (did we get the right number?),\n",
    "     - **completeness** (did we actually solve the problem with meaningful structure?), and\n",
    "     - **format bonus** (are we respecting Dual‑Stream tags and headings?)\n",
    "     into a single scalar.\n",
    "\n",
    "During GRPO, all three rewards are **added together** to produce a single\n",
    "reward per sampled rollout. That reward is what drives the policy updates.\n",
    "\n",
    "In practice, you can view DSA‑CAST here as a **grading rubric** for the DSA style:\n",
    "the SFT stage teaches the model *how* to speak in that structure, and\n",
    "DSA‑CAST + GRPO teaches it to speak **better, more consistently, and more correctly**\n",
    "while keeping Plan / Reasoning / Evidence / Sanity_check intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SMOKE_TEST:\n",
    "    print('Smoke test: skipping cell 33.')\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SMOKE_TEST:\n",
    "    print('Smoke test: skipping cell 34.')\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SMOKE_TEST:\n",
    "    print('Smoke test: skipping cell 35.')\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SMOKE_TEST:\n",
    "    print('Smoke test: skipping cell 36.')\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SMOKE_TEST:\n",
    "    print('Smoke test: skipping cell 37.')\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SMOKE_TEST:\n",
    "    print('Smoke test: skipping cell 38.')\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SMOKE_TEST:\n",
    "    print('Smoke test: skipping cell 39.')\n",
    "else:\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpuV5e8",
   "dataSources": [
    {
     "databundleVersionId": 14363498,
     "isSourceIdPinned": false,
     "sourceId": 119261,
     "sourceType": "competition"
    },
    {
     "datasetId": 4054119,
     "isSourceIdPinned": false,
     "sourceId": 7045423,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8992463,
     "sourceId": 14116074,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9014206,
     "sourceId": 14144381,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 222398,
     "modelInstanceId": 239467,
     "sourceId": 282742,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31194,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
