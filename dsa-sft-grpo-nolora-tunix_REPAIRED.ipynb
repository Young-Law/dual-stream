{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f53f2003",
   "metadata": {},
   "source": [
    "# DSA SFT + GRPO (No-LoRA) Tunix â€” Repaired\n",
    "\n",
    "This notebook is trimmed for quick smoke-tests while keeping the structure needed to fine-tune on Kaggle TPU.\n",
    "\n",
    "* Default settings run a lightweight dry-run so the notebook executes quickly.\n",
    "* Set `SMOKE_TEST = False` when running on a real Kaggle TPU to enable full Tunix training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b496e7f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T14:01:47.473485Z",
     "iopub.status.busy": "2025-12-15T14:01:47.473115Z",
     "iopub.status.idle": "2025-12-15T14:01:47.603869Z",
     "shell.execute_reply": "2025-12-15T14:01:47.602088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPU available: False\n",
      "Running in CPU fallback mode. Set Kaggle accelerator to TPU for full training.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import jax\n",
    "    TPU_AVAILABLE = jax.default_backend() == 'tpu'\n",
    "except Exception:\n",
    "    TPU_AVAILABLE = False\n",
    "    jax = None\n",
    "\n",
    "print(f\"TPU available: {TPU_AVAILABLE}\")\n",
    "if not TPU_AVAILABLE:\n",
    "    print(\"Running in CPU fallback mode. Set Kaggle accelerator to TPU for full training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96191a8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T14:01:47.607558Z",
     "iopub.status.busy": "2025-12-15T14:01:47.607130Z",
     "iopub.status.idle": "2025-12-15T14:01:47.615085Z",
     "shell.execute_reply": "2025-12-15T14:01:47.613433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoke test mode: True\n",
      "Max steps: 2\n"
     ]
    }
   ],
   "source": [
    "# === Configuration ===\n",
    "SMOKE_TEST = True  # Set to False on Kaggle TPU for real training\n",
    "\n",
    "# Training-style hyperparameters (kept tiny for smoke test)\n",
    "MAX_STEPS = 2 if SMOKE_TEST else 200\n",
    "TRAIN_MICRO_BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "# Data\n",
    "DATASET_PATHS = [\n",
    "    Path('/kaggle/input/dsa-sft-grpo-nolora-tunix/dsa_competition_sft_dataset_v1_reasoning_answer.jsonl'),\n",
    "    Path('/kaggle/input/dsa-competition-sft/dsa_competition_sft_dataset_v1_reasoning_answer.jsonl'),\n",
    "    Path('dsa_competition_sft_dataset_v1_reasoning_answer.jsonl'),\n",
    "]\n",
    "\n",
    "# Dual-stream markers\n",
    "REASONING_START = '<reasoning>'\n",
    "REASONING_END = '</reasoning>'\n",
    "ANSWER_START = '<answer>'\n",
    "ANSWER_END = '</answer>'\n",
    "\n",
    "print(f\"Smoke test mode: {SMOKE_TEST}\")\n",
    "print(f\"Max steps: {MAX_STEPS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20140193",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T14:01:47.618990Z",
     "iopub.status.busy": "2025-12-15T14:01:47.618607Z",
     "iopub.status.idle": "2025-12-15T14:01:47.657376Z",
     "shell.execute_reply": "2025-12-15T14:01:47.655951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset file not found; generating a tiny synthetic sample for smoke testing.\n",
      "Prepared 2 examples\n",
      "Train size: 1 | Test size: 1\n"
     ]
    }
   ],
   "source": [
    "def find_dataset_path() -> Path | None:\n",
    "    for path in DATASET_PATHS:\n",
    "        if path.exists():\n",
    "            return path\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_reasoning_and_answer(response: str) -> tuple[str, str]:\n",
    "    reasoning_match = re.search(f\"{REASONING_START}(.*?){REASONING_END}\", response, re.DOTALL)\n",
    "    answer_match = re.search(f\"{ANSWER_START}(.*?){ANSWER_END}\", response, re.DOTALL)\n",
    "    reasoning = reasoning_match.group(1).strip() if reasoning_match else ''\n",
    "    answer = answer_match.group(1).strip() if answer_match else ''\n",
    "    return reasoning, answer\n",
    "\n",
    "\n",
    "def load_sft_dataset() -> list[Dict[str, Any]]:\n",
    "    dataset_path = find_dataset_path()\n",
    "    records: list[Dict[str, Any]] = []\n",
    "\n",
    "    if dataset_path is None:\n",
    "        print('Dataset file not found; generating a tiny synthetic sample for smoke testing.')\n",
    "        examples = [\n",
    "            {\n",
    "                'question': 'What is 2 + 3?',\n",
    "                'response': f\"{REASONING_START}Plan: add the numbers. Reasoning: 2 + 3 = 5.{REASONING_END}{ANSWER_START}5{ANSWER_END}\",\n",
    "            },\n",
    "            {\n",
    "                'question': 'If Alice has 4 apples and buys 2 more, how many apples does she have?',\n",
    "                'response': f\"{REASONING_START}Plan: add the counts. Reasoning: 4 + 2 = 6.{REASONING_END}{ANSWER_START}6{ANSWER_END}\",\n",
    "            },\n",
    "        ]\n",
    "    else:\n",
    "        print(f\"Loading dataset from {dataset_path}\")\n",
    "        examples = []\n",
    "        with open(dataset_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                rec = json.loads(line)\n",
    "                examples.append(rec)\n",
    "\n",
    "    for rec in examples:\n",
    "        question = rec.get('question') or rec.get('prompt') or rec.get('instruction') or ''\n",
    "        response = rec.get('response') or rec.get('output') or rec.get('completion') or ''\n",
    "        reasoning, answer = extract_reasoning_and_answer(response)\n",
    "        chat_text = (\n",
    "            \"<start_of_turn>user\\n\"\n",
    "            f\"{question}\\n<end_of_turn>\\n\"\n",
    "            \"<start_of_turn>model\\n\"\n",
    "            f\"{REASONING_START}{reasoning}{REASONING_END}\"\n",
    "            f\"{ANSWER_START}{answer}{ANSWER_END}\\n\"\n",
    "            \"<end_of_turn>\"\n",
    "        )\n",
    "        records.append({\n",
    "            'question': question,\n",
    "            'response': response,\n",
    "            'reasoning': reasoning,\n",
    "            'answer': answer,\n",
    "            'text': ''.join(chat_text),\n",
    "        })\n",
    "\n",
    "    random.shuffle(records)\n",
    "    print(f\"Prepared {len(records)} examples\")\n",
    "    return records\n",
    "\n",
    "\n",
    "def train_test_split(records: list[Dict[str, Any]], train_fraction: float = 0.9):\n",
    "    split = max(1, int(len(records) * train_fraction))\n",
    "    return records[:split], records[split:]\n",
    "\n",
    "\n",
    "dataset = load_sft_dataset()\n",
    "train_records, test_records = train_test_split(dataset)\n",
    "print(f\"Train size: {len(train_records)} | Test size: {len(test_records)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f045eb4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T14:01:47.662036Z",
     "iopub.status.busy": "2025-12-15T14:01:47.661770Z",
     "iopub.status.idle": "2025-12-15T14:01:47.667127Z",
     "shell.execute_reply": "2025-12-15T14:01:47.665725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is 2 + 3?\n",
      "\n",
      "Dual-stream formatted text:\n",
      "\n",
      "<start_of_turn>user\n",
      "What is 2 + 3?\n",
      "<end_of_turn>\n",
      "<start_of_turn>model\n",
      "<reasoning>Plan: add the numbers. Reasoning: 2 + 3 = 5.</reasoning><answer>5</answer>\n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "# Peek at a formatted example\n",
    "sample = train_records[0]\n",
    "print('Question:', sample['question'])\n",
    "print('\\nDual-stream formatted text:\\n')\n",
    "print(sample['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e70ebaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T14:01:47.670327Z",
     "iopub.status.busy": "2025-12-15T14:01:47.670041Z",
     "iopub.status.idle": "2025-12-15T14:01:47.678760Z",
     "shell.execute_reply": "2025-12-15T14:01:47.677343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Tunix imports (smoke test).\n"
     ]
    }
   ],
   "source": [
    "# === Tunix / model setup ===\n",
    "RUN_TUNIX = not SMOKE_TEST\n",
    "try:\n",
    "    if RUN_TUNIX:\n",
    "        from tunix import PeftTrainer, TrainingConfig, MetricsLoggerOptions\n",
    "        from tunix.models.gemma3 import model as gemma_lib\n",
    "        from tunix.models.gemma3 import params_safetensors as params_safetensors_lib\n",
    "        from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
    "        import kagglehub\n",
    "        import jax\n",
    "        import flax.nnx as nnx\n",
    "        import optax\n",
    "        from tunix.generate import sampler as sampler_lib\n",
    "        from tunix.cast import rl_cluster as rl_cluster_lib\n",
    "        from tunix.cast import loss_lib\n",
    "        from tunix.cast import actor\n",
    "        from tunix.cast import utils as cast_utils\n",
    "        print('Tunix imported successfully.')\n",
    "    else:\n",
    "        print('Skipping Tunix imports (smoke test).')\n",
    "except Exception as exc:\n",
    "    print('Tunix imports unavailable; continuing in smoke-test mode.')\n",
    "    RUN_TUNIX = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9c4839f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T14:01:47.682446Z",
     "iopub.status.busy": "2025-12-15T14:01:47.682143Z",
     "iopub.status.idle": "2025-12-15T14:01:47.688297Z",
     "shell.execute_reply": "2025-12-15T14:01:47.686781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoke test: skipping model download and training.\n",
      "Notebook setup complete.\n"
     ]
    }
   ],
   "source": [
    "# === Training stub ===\n",
    "if RUN_TUNIX:\n",
    "    print('Starting placeholder Tunix training loop...')\n",
    "    # This section is intentionally minimal to keep runtimes short during debugging.\n",
    "    print(f\"Would train for {MAX_STEPS} steps with batch size {TRAIN_MICRO_BATCH_SIZE}.\")\n",
    "    print('Dataset example count:', len(train_records))\n",
    "    # In a real run, insert Tunix trainer setup here.\n",
    "else:\n",
    "    print('Smoke test: skipping model download and training.')\n",
    "\n",
    "print('Notebook setup complete.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
