{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":119261,"databundleVersionId":14363498,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":7045423,"sourceType":"datasetVersion","datasetId":4054119,"isSourceIdPinned":false},{"sourceId":14116074,"sourceType":"datasetVersion","datasetId":8992463},{"sourceId":14144381,"sourceType":"datasetVersion","datasetId":9014206},{"sourceId":282742,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":239467,"modelId":222398}],"dockerImageVersionId":31194,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gemma 3 (1B‚ÄëIT) Dual‚ÄëStream Training ‚Äì **SFT ‚Üí GRPO (DSA‚ÄëCAST, No‚ÄëLoRA)**\n\nThis notebook glues together two workflows into a **single, end‚Äëto‚Äëend training pipeline** on Gemma 3‚Äë1B‚ÄëIT:\n\n1. **Supervised Fine‚ÄëTuning (SFT)** ‚Äì teach the model to answer math questions in a **structured Dual‚ÄëStream** format.\n2. **GRPO (Group Relative Policy Optimization)** ‚Äì further **align** the model to that format and reward correctness and structure.\n\nTraining is **full‚Äëparameter** in both stages (no LoRA adapters).\n\n---\n\n## The DSA monologue structure\n\nHere, ‚ÄúDSA‚Äù is a **Dual‚ÄëStream Architecture**-based answering pattern with an internal monologue that is explicitly structured into four named sections:\n\nInside the `<reasoning>...</reasoning>` block, the model must always write:\n\n- **Plan** ‚Äì high‚Äëlevel steps it will take to solve the problem.  \n- **Reasoning** ‚Äì detailed step‚Äëby‚Äëstep execution.  \n- **Evidence** ‚Äì citations, calculations, and explicit checks that support the reasoning.  \n- **Sanity_check** ‚Äì a quick check that the final answer ‚Äúmakes sense‚Äù (magnitude, units, edge‚Äëcases).\n\nThen, outside the monologue, the model must put the final result in a separate `<answer>...</answer>` block:\n\n```text\n<reasoning>\nPlan:\n  ...\n\nReasoning:\n  ...\n\nEvidence:\n  ...\n\nSanity_check:\n  ...\n</reasoning>\n<answer>\n  42\n</answer>\n```\n\nThis gives you:\n\n- A **human‚Äëreadable monologue stream** for oversight and debugging.\n- A **machine‚Äëreadable answer stream** for automatic grading and downstream tools.\n\nFor the conceptual motivation and design details, see the accompanying whitepaper:  \n[The Inner Monologue: A Dual‚ÄëStream Architecture for Verifiable Inner Alignment](https://docs.google.com/document/d/1np-I9zEKArodlDhQzfydhloCXIVK9O72g3OJSuo_-Wk/edit?usp=sharing)\n\n---\n\n## How this notebook is organized\n\n1. **Part 1 ‚Äì SFT (Structured Dual‚ÄëStream Supervised Fine‚ÄëTuning)**\n   - Load Gemma 3‚Äë1B‚ÄëIT via Kaggle/Tunix (no HF token needed).\n   - Format GSM8K into the new DSA template:\n     - `<reasoning>` block with **Plan / Reasoning / Evidence / Sanity_check** sections.\n     - Separate `<answer>` block with only the final scalar.\n   - Train with SFT (no LoRA).\n   - Optionally do a quick post‚ÄëSFT generation sanity‚Äëcheck.\n   - **Zip and clean up the SFT checkpoints** so you keep a single artifact.\n\n2. **Part 2 ‚Äì GRPO (DSA‚ÄëCAST Reinforcement Learning)**\n   - Re‚Äëbuild a GSM8K‚Äëstyle dataset for RL rollouts using the same template.\n   - Define **DSA‚ÄëCAST rewards** that look at:\n     - Dual‚ÄëStream tags,\n     - Plan/Reasoning/Evidence/Sanity_check structure,\n     - and math correctness/completeness.\n   - Run GRPO with Tunix‚Äô `RLCluster` + `GRPOLearner` (no LoRA).\n   - Evaluate before/after GRPO on GSM8K.\n   - Export the **final GRPO actor checkpoint as a single zip** and clean up.\n\nBy default, the hyperparameters are set for a **debug‚Äëscale run** so you can validate wiring and behavior.  \nOnce you‚Äôre satisfied, you can increase `MAX_STEPS` etc. for a longer training run.","metadata":{}},{"cell_type":"markdown","source":"## Part 1 ‚Äî Supervised Fine‚ÄëTuning (SFT): Teaching the DSA Monologue\n\nThis section is the original **SFT notebook**, lightly edited:\n\n- It uses GSM8K to teach the model to respond with a structured monologue inside `<reasoning>...</reasoning>` containing:\n  - Plan\n  - Reasoning\n  - Evidence\n  - Sanity_check\n- It keeps a separate `<answer>...</answer>` block for the final scalar answer.\n- Hyperparameters are reduced so that training runs quickly.\n- At the end of Part 1, we zip the SFT checkpoints and clean up their directory.","metadata":{}},{"cell_type":"code","source":"import jax\nimport jax.numpy as jnp\nimport os\n\nprint(f\"JAX version: {jax.__version__}\")\nprint(f\"Number of devices: {len(jax.devices())}\")\nprint(f\"Device kind: {jax.devices()[0].device_kind}\")\nprint(f\"JAX backend: {jax.default_backend()}\")\nprint(f\"\\nDevices:\")\nfor i, device in enumerate(jax.devices()):\n    print(f\"  [{i}] {device}\")\nprint(\"=\"*60)\n\nif jax.default_backend() != 'tpu':\n    print(\"\\n‚ö†Ô∏è  WARNING: Not running on TPU!\")\n    print(f\"   Current backend: {jax.default_backend()}\")\n    print(\"   Make sure you've selected TPU runtime in Kaggle\")\nelse:\n    print(\"\\n‚úì TPU backend confirmed\")\n\n\nos.environ['XLA_FLAGS'] = (\n    '--xla_gpu_enable_triton_softmax_fusion=true '\n    '--xla_gpu_triton_gemm_any=True '\n    '--xla_gpu_enable_async_collectives=true'\n)\nos.environ['JAX_COMPILATION_CACHE_DIR'] = '/tmp/jax_cache'\nos.environ['LIBTPU_INIT_ARGS'] = '--xla_enable_async_all_gather=true'\n\njax.config.update('jax_enable_x64', False)  # Use 32-bit for speed\njax.config.update('jax_default_matmul_precision', 'high')  # BF16 matmuls\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T21:07:49.767972Z","iopub.execute_input":"2025-12-14T21:07:49.768197Z","iopub.status.idle":"2025-12-14T21:08:07.121655Z","shell.execute_reply.started":"2025-12-14T21:07:49.768180Z","shell.execute_reply":"2025-12-14T21:08:07.120290Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/site-packages/jax/_src/cloud_tpu_init.py:93: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"JAX version: 0.8.0\n","output_type":"stream"},{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1765746479.208854      12 common_lib.cc:648] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:238\n","output_type":"stream"},{"name":"stdout","text":"Number of devices: 8\nDevice kind: TPU v5 lite\nJAX backend: tpu\n\nDevices:\n  [0] TPU_0(process=0,(0,0,0,0))\n  [1] TPU_1(process=0,(1,0,0,0))\n  [2] TPU_2(process=0,(0,1,0,0))\n  [3] TPU_3(process=0,(1,1,0,0))\n  [4] TPU_4(process=0,(0,2,0,0))\n  [5] TPU_5(process=0,(1,2,0,0))\n  [6] TPU_6(process=0,(0,3,0,0))\n  [7] TPU_7(process=0,(1,3,0,0))\n============================================================\n\n‚úì TPU backend confirmed\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\nKAGGLE_MODEL_HANDLE = \"google/gemma-3/transformers/gemma-3-1b-it\"\n\nMAX_SEQ_LENGTH = 2048\nMESH_SHAPE = (1, 4) \nTRAIN_MICRO_BATCH_SIZE = 2 \n\nGRADIENT_ACCUMULATION_STEPS = 4 \n\nLEARNING_RATE = 2e-5 \n    \nNUM_EPOCHS = 1  # DEBUG: 1 epoch for quick sanity check       \nMAX_STEPS = 50\n#MAX_STEPS = 3500  # DEBUG: cap total SFT steps for quick run \nWARMUP_STEPS = int(0.1 * MAX_STEPS)\n\nADAM_BETA1 = 0.9\n\nADAM_BETA2 = 0.999 \n\nADAM_EPSILON = 1e-8\n\n\nWEIGHT_DECAY = 0.01 \nMAX_GRAD_NORM = 1.0\n\nprint(f\"Global Batch Size: {TRAIN_MICRO_BATCH_SIZE * 8 * GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"Total Training Steps: {MAX_STEPS}\")\n\n\nCHECKPOINT_DIR = \"/kaggle/working/outputs_sft_full/checkpoints\"\nTENSORBOARD_DIR = \"/kaggle/working/outputs_sft_full/tensorboard\"\nSAVE_INTERVAL_STEPS = 100\nEVAL_INTERVAL_STEPS = 50\nLOG_INTERVAL_STEPS = 10\n\nprint(\"‚úì Configuration loaded\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T21:08:07.122286Z","iopub.execute_input":"2025-12-14T21:08:07.122575Z","iopub.status.idle":"2025-12-14T21:08:07.127517Z","shell.execute_reply.started":"2025-12-14T21:08:07.122542Z","shell.execute_reply":"2025-12-14T21:08:07.126572Z"}},"outputs":[{"name":"stdout","text":"Global Batch Size: 64\nTotal Training Steps: 50\n‚úì Configuration loaded\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import kagglehub\nfrom tunix.models.gemma3 import model as gemma_lib\nfrom tunix.models.gemma3 import params_safetensors as params_safetensors_lib\nfrom tunix.generate import tokenizer_adapter as tokenizer_lib\n\nprint(f\"Model handle: {KAGGLE_MODEL_HANDLE}\")\n\nlocal_model_path = kagglehub.model_download(KAGGLE_MODEL_HANDLE)\nprint(f\"‚úì Model downloaded to: {local_model_path}\")\n\nprint(f\"\\nCreating TPU mesh with shape {MESH_SHAPE}...\")\nmesh = jax.make_mesh(MESH_SHAPE, ('fsdp', 'tp'))\nprint(f\"‚úì TPU Mesh created successfully\")\nprint(f\"  Mesh shape: {mesh.shape}\")\nprint(f\"  Mesh axis names: {mesh.axis_names}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T21:08:07.128319Z","iopub.execute_input":"2025-12-14T21:08:07.128490Z","iopub.status.idle":"2025-12-14T21:08:24.630014Z","shell.execute_reply.started":"2025-12-14T21:08:07.128474Z","shell.execute_reply":"2025-12-14T21:08:24.628854Z"}},"outputs":[{"name":"stdout","text":"Model handle: google/gemma-3/transformers/gemma-3-1b-it\n‚úì Model downloaded to: /kaggle/input/gemma-3/transformers/gemma-3-1b-it/1\n\nCreating TPU mesh with shape (1, 4)...\n‚úì TPU Mesh created successfully\n  Mesh shape: OrderedDict({'fsdp': 1, 'tp': 4})\n  Mesh axis names: ('fsdp', 'tp')\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\nmodel_config = gemma_lib.ModelConfig.gemma3_1b()\n\ngemma3_model = params_safetensors_lib.create_model_from_safe_tensors(\n    local_model_path,  # Directory containing .safetensors files\n    model_config,\n    mesh,\n)\nprint(\"‚úì Model loaded successfully\")\n\n\ntokenizer = tokenizer_lib.Tokenizer(\n    tokenizer_path=f\"{local_model_path}/tokenizer.model\"\n)\nprint(\"‚úì Tokenizer loaded successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T21:08:24.630563Z","iopub.execute_input":"2025-12-14T21:08:24.630843Z","iopub.status.idle":"2025-12-14T21:08:50.741284Z","shell.execute_reply.started":"2025-12-14T21:08:24.630825Z","shell.execute_reply":"2025-12-14T21:08:50.739907Z"}},"outputs":[{"name":"stdout","text":"‚úì Model loaded successfully\n‚úì Tokenizer loaded successfully\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import flax.nnx as nnx\n\n\nmodel_input = gemma3_model.get_model_input()\n\nprint(\"\\nSharding model across TPU devices...\")\nwith mesh:\n    state = nnx.state(gemma3_model)\n    pspecs = nnx.get_partition_spec(state)\n    sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n    nnx.update(gemma3_model, sharded_state)\n    \n    # Force materialization on TPU\n    _ = jax.tree_util.tree_map(lambda x: x.shape if hasattr(x, 'shape') else x, state)\n    \n\n\ntotal_params = sum(p.size for p in jax.tree_util.tree_leaves(nnx.state(gemma3_model)))\n\nprint(f\"\\n‚úì Model ready for full fine-tuning\")\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Trainable parameters: {total_params:,}\")\n\ndef format_chat_record_for_gemma(rec):\n    msgs = rec.get(\"messages\", [])\n    sys_msg = next((m.get(\"content\", \"\") for m in msgs if m.get(\"role\") == \"system\"), \"\")\n    user_msg = next((m.get(\"content\", \"\") for m in msgs if m.get(\"role\") == \"user\"), \"\")\n    asst_msg = next((m.get(\"content\", \"\") for m in msgs if m.get(\"role\") == \"assistant\"), \"\")\n\n    text = f\"\"\"<start_of_turn>user\n{sys_msg}\n\n{user_msg}<end_of_turn>\n<start_of_turn>model\n{asst_msg}\n<end_of_turn>\"\"\"\n\n    return {\"text\": text}\nall_params = nnx.state(gemma3_model)\nparam_leaves = jax.tree_util.tree_leaves(all_params)\nprint(f\"Number of parameters: {len(param_leaves)}\")\n\nif len(param_leaves) > 0:\n    sample = param_leaves[0]\n    print(f\"Sample param shape: {sample.shape}\")\n    print(f\"Sample param dtype: {sample.dtype}\")\n    \n    # Check device placement\n    if hasattr(sample, 'devices'):\n        devices_set = sample.devices()\n        print(f\"Sample param devices: {list(devices_set)}\")\n        if len(devices_set) > 0:\n            dev = list(devices_set)[0]\n            device_kind = dev.device_kind\n            print(f\"Device kind: {device_kind}\")\n            if 'tpu' in device_kind.lower():\n                print(\"‚úì‚úì‚úì SUCCESS: Model parameters are on TPU!\")\n                print(f\"‚úì‚úì‚úì Confirmed: {device_kind} detected\")\n            else:\n                print(f\"‚ùå‚ùå‚ùå ERROR: Model parameters are on {device_kind}, NOT TPU!\")\n                print(\"Training will run on CPU and produce wrong results!\")\n    else:\n        print(\"‚ö†Ô∏è  Cannot determine device placement\")\nelse:\n    print(\"‚ùå NO parameters found!\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T21:08:50.742010Z","iopub.execute_input":"2025-12-14T21:08:50.742188Z","iopub.status.idle":"2025-12-14T21:08:51.533770Z","shell.execute_reply.started":"2025-12-14T21:08:50.742170Z","shell.execute_reply":"2025-12-14T21:08:51.532440Z"}},"outputs":[{"name":"stdout","text":"\nSharding model across TPU devices...\n\n‚úì Model ready for full fine-tuning\nTotal parameters: 999,885,952\nTrainable parameters: 999,885,952\nNumber of parameters: 314\nSample param shape: (262144, 1152)\nSample param dtype: bfloat16\nSample param devices: [TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)]\nDevice kind: TPU v5 lite\n‚úì‚úì‚úì SUCCESS: Model parameters are on TPU!\n‚úì‚úì‚úì Confirmed: TPU v5 lite detected\n============================================================\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def format_chat_record_for_gemma(rec):\n    msgs = rec.get(\"messages\", [])\n    sys_msg = next((m.get(\"content\", \"\") for m in msgs if m.get(\"role\") == \"system\"), \"\")\n    user_msg = next((m.get(\"content\", \"\") for m in msgs if m.get(\"role\") == \"user\"), \"\")\n    asst_msg = next((m.get(\"content\", \"\") for m in msgs if m.get(\"role\") == \"assistant\"), \"\")\n\n    text = f\"\"\"<start_of_turn>user\n{sys_msg}\n\n{user_msg}<end_of_turn>\n<start_of_turn>model\n{asst_msg}\n<end_of_turn>\"\"\"\n\n    return {\"text\": text}\nimport re\nfrom datasets import load_dataset\nreasoning_start = \"<reasoning>\"\nreasoning_end = \"</reasoning>\"\nsolution_start = \"<answer>\"\nsolution_end = \"</answer>\"\n\nSYSTEM_PROMPT = \"\"\"You are a careful math tutor. You MUST respond in a Dual‚ÄëStream format.\n\nInside the <reasoning>...</reasoning> block, always structure your thoughts into four sections:\n1. Plan: high‚Äëlevel steps you will take to solve the problem.\n2. Reasoning: detailed step‚Äëby‚Äëstep execution.\n3. Evidence: citations, calculations, or explicit checks that support the reasoning.\n4. Sanity_check: a brief check that the final answer makes sense (magnitude, units, edge‚Äëcases).\n\nAfter the reasoning block, put ONLY the final numeric result inside <answer>...</answer>.\nThe final answer must appear exactly once in <answer>...</answer>.\n\"\"\"\n\n\n\nPROMPT_TEMPLATE = \"\"\"<start_of_turn>user\n{system_instruction}\n\n{question}<end_of_turn>\n<start_of_turn>model\n\"\"\"\ndef format_chat_record_for_gemma(rec):\n    msgs = rec.get(\"messages\", [])\n    sys_msg = next((m.get(\"content\", \"\") for m in msgs if m.get(\"role\") == \"system\"), \"\")\n    user_msg = next((m.get(\"content\", \"\") for m in msgs if m.get(\"role\") == \"user\"), \"\")\n    asst_msg = next((m.get(\"content\", \"\") for m in msgs if m.get(\"role\") == \"assistant\"), \"\")\n\n    text = f\"\"\"<start_of_turn>user\n{sys_msg}\n\n{user_msg}<end_of_turn>\n<start_of_turn>model\n{asst_msg}\n<end_of_turn>\"\"\"\n\n    return {\"text\": text}\n\n\nFULL_TEMPLATE = \"\"\"<start_of_turn>user\n{system_prompt}\n\n{question}<end_of_turn>\n<start_of_turn>model\n\n{reasoning_start}\n{reasoning}\n{reasoning_end}\n\n{solution_start}\n{answer}\n{solution_end}<end_of_turn>\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T21:10:09.895490Z","iopub.execute_input":"2025-12-14T21:10:09.895820Z","iopub.status.idle":"2025-12-14T21:10:11.147818Z","shell.execute_reply.started":"2025-12-14T21:10:09.895801Z","shell.execute_reply":"2025-12-14T21:10:11.146675Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Helper function to extract answer from GSM8K format\ndef extract_hash_answer(text):\n    \"\"\"Extract numerical answer after #### delimiter.\"\"\"\n    if \"####\" not in text:\n        return None\n    return text.split(\"####\")[1].strip()\n\n# Helper function to extract reasoning from GSM8K format\ndef extract_reasoning(text):\n    \"\"\"Extract reasoning (everything before #### delimiter).\"\"\"\n    if \"####\" not in text:\n        def format_chat_record_for_gemma(rec):\n            msgs = rec.get(\"messages\", [])\n            sys_msg = next((m.get(\"content\", \"\") for m in msgs if m.get(\"role\") == \"system\"), \"\")\n            user_msg = next((m.get(\"content\", \"\") for m in msgs if m.get(\"role\") == \"user\"), \"\")\n            asst_msg = next((m.get(\"content\", \"\") for m in msgs if m.get(\"role\") == \"assistant\"), \"\")\n        \n    text = f\"\"\"<start_of_turn>user\n{sys_msg}\n\n{user_msg}<end_of_turn>\n<start_of_turn>model\n{asst_msg}\n<end_of_turn>\"\"\"\n\n    return {\"text\": text}\n        return text.strip()\n    return text.split(\"####\")[0].strip()\n\n# Load GSM8K dataset\nprint(\"Loading GSM8K dataset...\")\ntrain_dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")\ntest_dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\nprint(f\"‚úì Loaded {len(train_dataset)} training examples\")\nprint(f\"‚úì Loaded {len(test_dataset)} test examples\")\n\n\nprint(\"\\nExample question:\")\nprint(train_dataset[0][\"question\"])\nprint(\"\\nExample answer:\")\nprint(train_dataset[0][\"answer\"])\nprint(\"\\nExtracted reasoning:\")\nprint(extract_reasoning(train_dataset[0][\"answer\"]))\nprint(\"\\nExtracted numerical answer:\")\nprint(extract_hash_answer(train_dataset[0][\"answer\"]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\nimport re\nimport os\nimport json\nimport random\nfrom collections import defaultdict\nimport glob\nreasoning_start = \"<reasoning>\"\nreasoning_end = \"</reasoning>\"\nsolution_start = \"<answer>\"\nsolution_end = \"</answer>\"\n\n\n# 1. Define the Cleaning Helper\ndef clean_gsm8k_content(text):\n    \"\"\"\n    Removes GSM8K specific calculation anndef _format_chat_record_for_gemma(rec):\n    msgs = rec.get(\"messages\", [])\n    sys_msg = next((m.get(\"content\", \"\") for m in msgs if m.get(\"role\") == \"system\"), \"\")\n    user_msg = next((m.get(\"content\", \"\") for m in msgs if m.get(\"role\") == \"user\"), \"\")\n    asst_msg = next((m.get(\"content\", \"\") for m in msgs if m.get(\"role\") == \"assistant\"), \"\")\n\n    text = f\"\"\"<start_of_turn>user\n{sys_msg}\n\n{user_msg}<end_of_turn>\n<start_of_turn>model\n{asst_msg}\n<end_of_turn>\"\"\"\n\n    return {\"text\": text}otations.\n    Converts '<<10+5=15>>' to '(10+5=15)' or just removes them if preferred.\n    \"\"\"\n    # Remove GSM8K-style '<<...>>' annotations\n    text = re.sub(r\"<<(.*?)>>\", r\"(\\1)\", text)\n    # Normalize spacing\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\n\ndef format_gsm8k_example(ex):\n    # Raw fields\n    question = ex[\"question\"]\n    raw_answer = ex[\"answer\"]\n\n    # Split GSM8K-style answer into reasoning and final numeric answer\n    # Format is usually \"... explanation ... #### 42\"\n    if \"####\" in raw_answer:\n        reasoning_raw, answer_raw = raw_answer.split(\"####\", 1)\n        reasoning = clean_gsm8k_content(reasoning_raw.strip())\n        answer = answer_raw.strip()\n    else:\n        reasoning = clean_gsm8k_content(raw_answer.strip())\n        answer = raw_answer.strip()\n\n    # 1. User Turn (Includes the strict instructions)\n    text = f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\\n{question}<end_of_turn>\\n\"\n\n    # 2. Model Turn (Structured DSA monologue + final answer)\n    plan_section = (\n        \"Plan:\\n\"\n        \"- We will break the problem into smaller steps and solve them one by one.\\n\"\n    )\n    reasoning_section = f\"Reasoning:\\n{reasoning}\\n\"\n    evidence_section = (\n        \"Evidence:\\n\"\n        \"- The calculations in the reasoning show each intermediate step explicitly.\\n\"\n    )\n    sanity_section = (\n        \"Sanity_check:\\n\"\n        f\"- The final answer {answer} should make sense given the quantities in the problem.\\n\"\n    )\n\n    text += \"<start_of_turn>model\\n\"\n    text += \"<reasoning>\\n\"\n    text += plan_section + \"\\n\"\n    text += reasoning_section + \"\\n\"\n    text += evidence_section + \"\\n\"\n    text += sanity_section\n    text += \"</reasoning>\\n\"\n    text += \"<answer>\\n\"\n    text += f\"{answer}\\n\"\n    text += \"</answer>\"\n    text += \"<end_of_turn>\"\n\n    return {\"text\": text}\n\n# Prefer the local multi-domain DSA dataset (JSONL) when present.\nDSA_JSONL_FILENAME = \"dsa_competition_sft_dataset_v1_reasoning_answer.jsonl\"\n# Try common locations (cwd, /kaggle/working, and any attached Kaggle input dataset folder).\nDSA_JSONL_CANDIDATES = [\n    DSA_JSONL_FILENAME,\n    f\"/kaggle/working/{DSA_JSONL_FILENAME}\",\n]\nDSA_JSONL_CANDIDATES += glob.glob(f\"/kaggle/input/**/{DSA_JSONL_FILENAME}\", recursive=True)\nDSA_JSONL_PATH = next((p for p in DSA_JSONL_CANDIDATES if os.path.exists(p)), DSA_JSONL_FILENAME)\n\nFORCE_USE_GSM8K = False  # set True if you explicitly want to train on GSM8K instead\n\ndef _load_dsa_jsonl(path, eval_ratio=0.1, seed=42):\n    rng = random.Random(seed)\n    raw = []\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            raw.append(json.loads(line))\n\n    # Stratified-ish split by category to keep eval coverage.\n    by_cat = defaultdict(list)\n    for r in raw:\n        by_cat[r.get(\"category\", \"unknown\")].append(r)\n\n    train, eval_ = [], []\n    for cat, items in by_cat.items():\n        rng.shuffle(items)\n        # allocate at least 1 to eval when category has 2+ items\n        n_eval = max(1, int(len(items) * eval_ratio)) if len(items) > 1 else 0\n        eval_.extend(items[:n_eval])\n        train.extend(items[n_eval:])\n\n    rng.shuffle(train)\n    rng.shuffle(eval_)\n    return train, eval_\n\ndef format_chat_record_for_gemma(rec):\n    msgs = rec.get(\"messages\", [])\n    sys_msg = next((m.get(\"content\", \"\") for m in msgs if m.get(\"role\") == \"system\"), \"\")\n    user_msg = next((m.get(\"content\", \"\") for m in msgs if m.get(\"role\") == \"user\"), \"\")\n    asst_msg = next((m.get(\"content\", \"\") for m in msgs if m.get(\"role\") == \"assistant\"), \"\")\n\n    text = f\"\"\"<start_of_turn>user\n{sys_msg}\n\n{user_msg}<end_of_turn>\n<start_of_turn>model\n{asst_msg}\n<end_of_turn>\"\"\"\n\n    return {\"text\": text}def format_chat_record_for_gemma(rec):\n    msgs = rec.get(\"messages\", [])\n    sys_msg = next((m.get(\"content\", \"\") for m in msgs if m.get(\"role\") == \"system\"), \"\")\n    user_msg = next((m.get(\"content\", \"\") for m in msgs if m.get(\"role\") == \"user\"), \"\")\n    asst_msg = next((m.get(\"content\", \"\") for m in msgs if m.get(\"role\") == \"assistant\"), \"\")\n\n    text = f\"\"\"<start_of_turn>user\n{sys_msg}\n\n{user_msg}<end_of_turn>\n<start_of_turn>model\n{asst_msg}\n<end_of_turn>\"\"\"\n\n    return {\"text\": text}\n\nif os.path.exists(DSA_JSONL_PATH) and not FORCE_USE_GSM8K:\n    print(f\"Loading local DSA JSONL dataset from {DSA_JSONL_PATH} ...\")\n    dsa_train, dsa_eval = _load_dsa_jsonl(DSA_JSONL_PATH, eval_ratio=0.1, seed=42)\n    formatted_train = [format_chat_record_for_gemma(r) for r in dsa_train]\n    formatted_test  = [format_chat_record_for_gemma(r) for r in dsa_eval]\n    print(f\"‚úì Loaded DSA JSONL: {len(formatted_train)} train, {len(formatted_test)} eval\")\nelse:\n    print(\"Refining dataset with CLEANING and structured DSA System Prompt...\")\n    train_dataset = load_dataset(\"openai/gsm8k\", \"main\")[\"train\"]\n    test_dataset = load_dataset(\"openai/gsm8k\", \"main\")[\"test\"]\n\n    formatted_train = [format_gsm8k_example(ex) for ex in train_dataset]\n    formatted_test = [format_gsm8k_example(ex) for ex in test_dataset]\n# Optionally augment with a small custom basic-math dataset (roots, percents, units).\n# Expected CSV schema:\n#   question,answer,reasoning,calc_expr\nimport os, csv\n\nCUSTOM_BASIC_MATH_PATH = \"dsa_basic_math_roots_percents_units.csv\"\ncustom_formatted = []\nif os.path.exists(CUSTOM_BASIC_MATH_PATH):\n    print(f\"Loading custom basic-math dataset from {CUSTOM_BASIC_MATH_PATH} ...\")\n    with open(CUSTOM_BASIC_MATH_PATH, newline=\"\", encoding=\"utf-8\") as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            q = row.get(\"question\", \"\").strip()\n            a = row.get(\"answer\", \"\").strip()\n            reasoning = row.get(\"reasoning\", \"\").strip()\n            calc_expr = row.get(\"calc_expr\", \"\").strip()\n\n            text = \"\"\"<start_of_turn>user\n{system_prompt}\n\n{question}<end_of_turn>\n<start_of_turn>model\n\"\"\".format(system_prompt=SYSTEM_PROMPT, question=q)\n\n            text += \"\\n<reasoning>\\n\"\n            text += \"Plan:\\n- We will break the problem into smaller steps and solve them one by one.\\n\\n\"\n            text += \"Reasoning:\\n\" + reasoning + \"\\n\\n\"\n\n            evidence_lines = [\"Evidence:\"]\n            if calc_expr:\n                evidence_lines.append(f\"- CALC: {calc_expr} = {a}\")\n            else:\n                evidence_lines.append(\"- The calculations in the reasoning show each intermediate step explicitly.\")\n            text += \"\\n\".join(evidence_lines) + \"\\n\\n\"\n\n            text += \"Sanity_check:\\n\"\n            text += f\"- The final answer {a} should make sense given the quantities in the problem.\\n\"\n            text += \"</reasoning>\\n\"\n            text += \"<answer>\\n\" + a + \"\\n</answer>\"\n            text += \"<end_of_turn>\"\n\n            custom_formatted.append({\"text\": text})\n\n    print(f\"Loaded {len(custom_formatted)} custom basic-math examples.\")\n    formatted_train.extend(custom_formatted)\nelse:\n    print(\"No custom basic-math CSV found; using GSM8K only.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T21:11:50.832531Z","iopub.execute_input":"2025-12-14T21:11:50.832876Z","iopub.status.idle":"2025-12-14T21:11:50.843194Z","shell.execute_reply.started":"2025-12-14T21:11:50.832848Z","shell.execute_reply":"2025-12-14T21:11:50.842412Z"}},"outputs":[{"traceback":["  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m{user_msg}<end_of_turn>\u001b[39m\n                           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (480592263.py, line 26)","output_type":"error"}],"execution_count":22},{"cell_type":"code","source":"print(\"-\" * 60)\nprint(formatted_train[100][\"text\"])\nprint(\"-\" * 60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T21:11:51.061664Z","iopub.execute_input":"2025-12-14T21:11:51.061871Z","iopub.status.idle":"2025-12-14T21:11:51.085216Z","shell.execute_reply.started":"2025-12-14T21:11:51.061844Z","shell.execute_reply":"2025-12-14T21:11:51.084307Z"}},"outputs":[{"name":"stdout","text":"------------------------------------------------------------\n","output_type":"stream"},{"traceback":["\u001b[31m---------------------------------------------------------------------------\u001b[39m","\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)","\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mformatted_train\u001b[49m[\u001b[32m100\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n","\u001b[31mNameError\u001b[39m: name 'formatted_train' is not defined"],"ename":"NameError","evalue":"name 'formatted_train' is not defined","output_type":"error"}],"execution_count":23},{"cell_type":"code","source":"import grain.python as grain\nimport numpy as np\nfrom tunix.sft import metrics_logger as tmetrics\nfrom tunix.sft.peft_trainer import TrainingInput\n# Force metrics_logger to behave as if wandb is unavailable\ntmetrics.wandb = None\n\ndef tokenize_function(example):\n    full_text = example[\"text\"]\n    full_tokens = tokenizer.encode(full_text)\n    \n    \n    prompt_text = full_text.split(\"<start_of_turn>model\")[0] + \"<start_of_turn>model\\n\"\n    prompt_tokens = tokenizer.encode(prompt_text)\n    prompt_len = len(prompt_tokens)\n\n    # Padding/Truncation Logic\n    if len(full_tokens) > MAX_SEQ_LENGTH:\n        full_tokens = full_tokens[:MAX_SEQ_LENGTH]\n    else:\n        pad_token = tokenizer.pad_id() if hasattr(tokenizer, 'pad_id') else tokenizer.eos_id()\n        full_tokens = full_tokens + [pad_token] * (MAX_SEQ_LENGTH - len(full_tokens))\n\n    input_tokens = np.array(full_tokens, dtype=np.int32)\n    \n    # Create Mask\n    loss_mask = np.zeros_like(input_tokens, dtype=np.float32)\n    \n    # Enable loss only for the response part (ignoring padding)\n    seq_len = min(len(tokenizer.encode(full_text)), MAX_SEQ_LENGTH)\n    if seq_len > prompt_len:\n        loss_mask[prompt_len:seq_len] = 1.0\n\n    return TrainingInput(input_tokens=input_tokens, input_mask=loss_mask)\n\n\n# Create Grain datasets\ntrain_grain = (\n    grain.MapDataset.source(formatted_train)\n    .map(tokenize_function)\n    .shuffle(seed=42)\n    .repeat(NUM_EPOCHS)\n    .batch(batch_size=TRAIN_MICRO_BATCH_SIZE, drop_remainder=True)\n)\n\neval_grain = (\n    grain.MapDataset.source(formatted_test)\n    .map(tokenize_function)\n    .batch(batch_size=TRAIN_MICRO_BATCH_SIZE, drop_remainder=True)\n)\n\nprint(f\"‚úì Train batches: {len(train_grain):,}\")\nprint(f\"‚úì Eval batches: {len(eval_grain):,}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T21:11:51.097188Z","iopub.execute_input":"2025-12-14T21:11:51.097381Z","iopub.status.idle":"2025-12-14T21:11:51.429188Z","shell.execute_reply.started":"2025-12-14T21:11:51.097364Z","shell.execute_reply":"2025-12-14T21:11:51.428011Z"}},"outputs":[{"traceback":["\u001b[31m---------------------------------------------------------------------------\u001b[39m","\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)","\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m TrainingInput(input_tokens=input_tokens, input_mask=loss_mask)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Create Grain datasets\u001b[39;00m\n\u001b[32m     38\u001b[39m train_grain = (\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     grain.MapDataset.source(\u001b[43mformatted_train\u001b[49m)\n\u001b[32m     40\u001b[39m     .map(tokenize_function)\n\u001b[32m     41\u001b[39m     .shuffle(seed=\u001b[32m42\u001b[39m)\n\u001b[32m     42\u001b[39m     .repeat(NUM_EPOCHS)\n\u001b[32m     43\u001b[39m     .batch(batch_size=TRAIN_MICRO_BATCH_SIZE, drop_remainder=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     44\u001b[39m )\n\u001b[32m     46\u001b[39m eval_grain = (\n\u001b[32m     47\u001b[39m     grain.MapDataset.source(formatted_test)\n\u001b[32m     48\u001b[39m     .map(tokenize_function)\n\u001b[32m     49\u001b[39m     .batch(batch_size=TRAIN_MICRO_BATCH_SIZE, drop_remainder=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     50\u001b[39m )\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úì Train batches: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_grain)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n","\u001b[31mNameError\u001b[39m: name 'formatted_train' is not defined"],"ename":"NameError","evalue":"name 'formatted_train' is not defined","output_type":"error"}],"execution_count":24},{"cell_type":"code","source":"import optax\neffective_warmup_steps = min(WARMUP_STEPS, max(1, MAX_STEPS - 1))\nschedule = optax.warmup_cosine_decay_schedule(\n    init_value=0.0,\n    peak_value=LEARNING_RATE,\n    warmup_steps=effective_warmup_steps,\n    decay_steps=MAX_STEPS - WARMUP_STEPS,\n    end_value=LEARNING_RATE * 0.1,\n)\n\n# Create optimizer chain\noptimizer = optax.chain(\n    optax.clip_by_global_norm(MAX_GRAD_NORM),\n    optax.scale_by_adam(\n        b1=ADAM_BETA1,\n        b2=ADAM_BETA2,\n        eps=ADAM_EPSILON,\n    ),\n    optax.add_decayed_weights(WEIGHT_DECAY),\n    optax.scale_by_schedule(schedule),\n    optax.scale(-1.0),  # Gradient descent\n)\n\nprint(\"‚úì Optimizer configured:\")\nprint(f\"  Learning rate: {LEARNING_RATE}\")\nprint(f\"  Warmup steps: {WARMUP_STEPS}\")\nprint(f\"  Total steps: {MAX_STEPS}\")\nprint(f\"  Weight decay: {WEIGHT_DECAY}\")\nprint(f\"  Max grad norm: {MAX_GRAD_NORM}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T21:11:51.429882Z","iopub.status.idle":"2025-12-14T21:11:51.430231Z","shell.execute_reply.started":"2025-12-14T21:11:51.429986Z","shell.execute_reply":"2025-12-14T21:11:51.429996Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tunix import PeftTrainer, TrainingConfig, MetricsLoggerOptions\nimport orbax.checkpoint as ocp\nfrom tunix.sft import metrics_logger as tmetrics\ntmetrics.wandb = None  # üëà add this once\n\n\ncheckpointing_options = ocp.CheckpointManagerOptions(\n    save_interval_steps=SAVE_INTERVAL_STEPS,\n    max_to_keep=3,  # Keep last 3 checkpoints\n)\n\ntraining_config = TrainingConfig(\n    max_steps=MAX_STEPS,\n    eval_every_n_steps=EVAL_INTERVAL_STEPS,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    checkpoint_root_directory=CHECKPOINT_DIR,\n    checkpointing_options=checkpointing_options,\n    metrics_logging_options=None,  # ‚úÖ disable W&B / monitoring\n)\n\n\nprint(\"‚úì Training configuration created\")\nprint(f\"  Max steps: {MAX_STEPS}\")\nprint(f\"  Micro batch size: {TRAIN_MICRO_BATCH_SIZE}\")\nprint(f\"  Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"  Effective batch size: {TRAIN_MICRO_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"  Eval interval: {EVAL_INTERVAL_STEPS}\")\nprint(f\"  Save interval: {SAVE_INTERVAL_STEPS}\")\n\n# Model input function\nfrom tunix.sft import utils\n\ndef gen_model_input_fn(training_input):\n    \"\"\"Convert TrainingInput to model-compatible format.\"\"\"\n    pad_mask = training_input.input_tokens != 0\n    positions = utils.build_positions_from_mask(pad_mask)\n    attention_mask = utils.make_causal_attn_mask(pad_mask)\n    \n    return {\n        'input_tokens': training_input.input_tokens,\n        'input_mask': training_input.input_mask,\n        'positions': positions,\n        'attention_mask': attention_mask,\n    }\n\n\ntrainer = PeftTrainer(\n    model=gemma3_model,\n    optimizer=optimizer,\n    training_config=training_config,\n)\ntrainer = trainer.with_gen_model_input_fn(gen_model_input_fn)\n\nprint(\"‚úì Trainer ready for training\")\nprint(f\"  Model: Gemma 3 1B (Full Fine-Tuning)\")\nprint(f\"  Max steps: {MAX_STEPS}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T21:11:51.430538Z","iopub.status.idle":"2025-12-14T21:11:51.431107Z","shell.execute_reply.started":"2025-12-14T21:11:51.430641Z","shell.execute_reply":"2025-12-14T21:11:51.430649Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Click **RUN > Run current and after**","metadata":{}},{"cell_type":"code","source":"import time\n\nprint(\"=\"*60)\nprint(\"Starting Full Fine-Tuning on TPU v5e-8\")\nprint(\"=\"*60)\nprint(f\"Max steps: {MAX_STEPS}\")\nprint(f\"Training examples: {len(formatted_train)}\")\nprint(f\"Eval examples: {len(formatted_test)}\")\nprint(f\"Batch size: {TRAIN_MICRO_BATCH_SIZE}\")\nprint(f\"Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\nprint(\"=\"*60)\n\n\nall_params = nnx.state(gemma3_model)\nparam_leaves = jax.tree_util.tree_leaves(all_params)\nif len(param_leaves) > 0:\n    sample_param = param_leaves[0]\n    if hasattr(sample_param, 'devices'):\n        devices = sample_param.devices()\n        if len(devices) > 0:\n            device_kind = list(devices)[0].device_kind\n            print(f\"‚úì Model parameters are on: {device_kind}\")\n            if 'tpu' not in device_kind.lower():\n                print(f\"‚ö†Ô∏è  WARNING: Model params on {device_kind}, not TPU!\")\n                print(f\"‚ö†Ô∏è  Training will run on CPU and produce wrong results!\")\n            else:\n                print(f\"‚úì‚úì‚úì CONFIRMED: Model is ready for TPU training!\")\n        else:\n            print(\"‚ö†Ô∏è  No devices found for model parameters\")\n    else:\n        print(\"‚ö†Ô∏è  Cannot check device placement\")\nelse:\n    print(\"‚ö†Ô∏è  No model parameters found\")\nprint(\"=\"*60)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"IMPORTANT: First training step will take 2-5 minutes\")\nprint(\"=\"*60)\nprint(\"JAX is compiling all functions (happens on CPU).\")\nprint(\"After first step completes, TPU will be used and steps will be MUCH faster.\")\nprint(\"You should see 'Compiling...' messages initially.\")\nprint(\"=\"*60)\n\nprint(\"\\nStarting training...\")\nstart_time = time.time()\n\n\ntrainer.train(\n    train_ds=train_grain,\n    eval_ds=eval_grain,\n)\n\nend_time = time.time()\ntotal_time = end_time - start_time\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Training Completed!\")\nprint(\"=\"*60)\nprint(f\"Total training time: {total_time:.1f} seconds ({total_time/60:.1f} minutes)\")\nprint(f\"Average time per step: {total_time/MAX_STEPS:.1f} seconds\")\nprint(f\"Checkpoints saved to: {CHECKPOINT_DIR}\")\nprint(\"=\"*60)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"POST-TRAINING: Verify TPU was used\")\nprint(\"=\"*60)\nprint(f\"Expected TPU time: 5-15 seconds per step after compilation\")\nprint(f\"Your average: {total_time/MAX_STEPS:.1f} seconds per step\")\nif total_time/MAX_STEPS < 1.0:\n    print(\"‚ùå WARNING: Training ran on CPU, not TPU!\")\n    print(\"Results will be incorrect. Check that model is properly sharded.\")\nelse:\n    print(\"‚úì Training timing looks correct for TPU usage!\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T21:11:51.461145Z","iopub.execute_input":"2025-12-14T21:11:51.461408Z","iopub.status.idle":"2025-12-14T21:11:51.499223Z","shell.execute_reply.started":"2025-12-14T21:11:51.461371Z","shell.execute_reply":"2025-12-14T21:11:51.498255Z"}},"outputs":[{"name":"stdout","text":"============================================================\nStarting Full Fine-Tuning on TPU v5e-8\n============================================================\nMax steps: 1500\n","output_type":"stream"},{"traceback":["\u001b[31m---------------------------------------------------------------------------\u001b[39m","\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)","\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMax steps: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_STEPS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining examples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43mformatted_train\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEval examples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(formatted_test)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBatch size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTRAIN_MICRO_BATCH_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n","\u001b[31mNameError\u001b[39m: name 'formatted_train' is not defined"],"ename":"NameError","evalue":"name 'formatted_train' is not defined","output_type":"error"}],"execution_count":25},{"cell_type":"code","source":"from tunix.generate import sampler as sampler_lib\nimport json\nimport os\n\n\ncache_config = sampler_lib.CacheConfig(\n    cache_size=MAX_SEQ_LENGTH + 512,\n    num_layers=model_config.num_layers,\n    num_kv_heads=model_config.num_kv_heads,\n    head_dim=model_config.head_dim,\n)\n\n\ngeneration_sampler = sampler_lib.Sampler(\n    transformer=gemma3_model,\n    tokenizer=tokenizer,\n    cache_config=cache_config,\n)\n\n\ndef generate_inference_prompt(question):\n    # Match the training exactly: Same System Prompt, No One-Shot needed anymore.\n    text = f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\\n{question}<end_of_turn>\\n\"\n    text += f\"<start_of_turn>model\\n<reasoning>\\n\" \n    return text\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T21:12:00.277692Z","iopub.execute_input":"2025-12-14T21:12:00.278035Z","iopub.status.idle":"2025-12-14T21:12:00.312298Z","shell.execute_reply.started":"2025-12-14T21:12:00.278012Z","shell.execute_reply":"2025-12-14T21:12:00.311267Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Test questions\ntest_questions = [\n    \"What is the square root of 144?\",\n    \"If a shirt costs $25 and is on sale for 20% off, what is the sale price?\",\n    \"A train travels 60 miles in 45 minutes. What is its speed in miles per hour?\",\n    \"What is 15% of 200?\",\n]\n\nprint(\"=\"*60)\nprint(\"Testing Trained Model (Strict Format)\")\nprint(\"=\"*60)\n\nfor i, question in enumerate(test_questions, 1):\n    # 1. Generate the formatted prompt\n    prompt = generate_inference_prompt(question)\n\n    print(f\"\\n[Test {i}] Question: {question}\")\n    print(\"-\" * 60)\n\n    # 2. Run Generation\n    sampler_output = generation_sampler(\n        input_strings=[prompt],\n        max_generation_steps=512,\n        temperature=0.01,  # Near-greedy for math\n        top_k=1,\n    )\n\n    # 3. Extract and Clean Response\n    response = sampler_output.text[0]\n    \n    # Manual Stop: Cut off text if the model generates <end_of_turn>\n    # This fixes the looping issue seen in Test 4\n    if \"<end_of_turn>\" in response:\n        response = response.split(\"<end_of_turn>\")[0]\n\n    print(f\"Response:\\n{response}\")\n    print(\"=\" * 60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T21:12:01.113609Z","iopub.execute_input":"2025-12-14T21:12:01.114042Z","iopub.status.idle":"2025-12-14T21:13:19.526827Z","shell.execute_reply.started":"2025-12-14T21:12:01.114010Z","shell.execute_reply":"2025-12-14T21:13:19.525598Z"}},"outputs":[{"name":"stdout","text":"============================================================\nTesting Trained Model (Strict Format)\n============================================================\n\n[Test 1] Question: What is the square root of 144?\n------------------------------------------------------------\nResponse:\nreason:\nThe square root of 144 is 12.\n\n</reasoning>\n<answer>\n12\n</answer>\n============================================================\n\n[Test 2] Question: If a shirt costs $25 and is on sale for 20% off, what is the sale price?\n------------------------------------------------------------\nResponse:\nreason:\nThe sale price is 25*.20 = $(25*.20=5)5 off. So the sale price is 25-5 = $(25-5=20)20.\n\n<answer>\n20\n</answer>\n============================================================\n\n[Test 3] Question: A train travels 60 miles in 45 minutes. What is its speed in miles per hour?\n------------------------------------------------------------\nResponse:\nPlan:\n- We will break the problem into smaller steps and solve them one by one.\nReasoning:\nFirst find the total number of minutes in 45 minutes: 45 minutes * 60 minutes/hour = (45*60=2700)2700 minutes Then divide the total number of minutes by the number of minutes per hour to find the total number of hours: 2700 minutes / 60 minutes/hour = (2700/60=45)45 hours Then divide the total number of hours by the number of hours per day to find the total number of days: 45 hours / 2 = (45/2=22.5)22.5 days Then divide the total number of days by the number of days per year to find the total number of years: 22.5 days / 365 days/year = (22.5/365=0.62)0.62 years Then divide the total number of years by the number of years per year to find the total number of years: 0.62 years * 365 days/year = (0.62*365=220.9)220.9 years Then divide the total number of years by the number of years per year to find the total number of years in a year: 220.9 years / 365 days/year = (220.9/365=0.62)0.62 years Then divide the total number of years in a year by the number of years in a year to find the total number of years in a year: 0.62 years * 365 days/year = (0.62*365=220.9)220.9 years Then divide the total number of years in a year by the number of years in a year to find the total number of years in a year: 220.9 years / 365 days/year = (220.9/365=0.62)0.62 years Then divide the total number of years in a year by the number of years in a year to find the total number of years in a year: 0.62 years * 36\n============================================================\n\n[Test 4] Question: What is 15% of 200?\n------------------------------------------------------------\nResponse:\nreason:\n15/100 * 200 = (15/100*200=30)30\n\n<answer>\n30</answer>\n============================================================\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import collections\nimport time\nimport re\nfrom tqdm.auto import tqdm\n\n\nVOTE_SAMPLES = 1 \n\n# Temperature must be > 0 to get diverse reasoning paths\n# 0.6 is standard for Self-Consistency\nTEMPERATURE = 0.7 \n\n# Max tokens for the answer\nMAX_GEN_STEPS = 512\n\nprint(\"\\n\" + \"=\"*60)\nprint(f\"Evaluating with Majority Voting (k={VOTE_SAMPLES})\")\nprint(\"=\"*60)\n\n\ndef normalize_answer(answer_str):\n    \"\"\"Normalize answer string for comparison.\"\"\"\n    if answer_str is None:\n        return None\n    s = str(answer_str).strip().lower()\n    s = s.replace('$', '').replace(',', '').replace('¬£', '').replace('‚Ç¨', '')\n    if s.endswith('.'):\n        s = s[:-1]\n    return s\n\ndef extract_answer_robust(response):\n    \"\"\"\n    Extracts answers using a cascade of patterns (XML -> Boxed -> Text).\n    \"\"\"\n    # 1. Try <answer> tags\n    xml_match = re.search(r\"<answer>\\s*(.*?)\\s*</answer>\", response, re.DOTALL)\n    if xml_match:\n        return xml_match.group(1)\n\n    # 2. Try LaTeX \\boxed{}\n    boxed_match = re.search(r\"\\\\boxed\\{([^}]+)\\}\", response)\n    if boxed_match:\n        return boxed_match.group(1)\n\n    # 3. Try \"Final Answer\" text patterns\n    text_match = re.search(r\"(?:final answer|answer is)[:\\s]*([0-9\\.]+)\", response, re.IGNORECASE)\n    if text_match:\n        return text_match.group(1)\n\n    # 4. Fallback: Last number\n    numbers = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", response)\n    if numbers:\n        return numbers[-1]\n    return None\n\ndef get_majority_vote(candidates):\n    \"\"\"Returns the most common answer from a list of candidates.\"\"\"\n    # Filter out None values\n    valid_candidates = [c for c in candidates if c is not None]\n    \n    if not valid_candidates:\n        return None\n    \n    # Count frequency\n    counter = collections.Counter(valid_candidates)\n    \n    # Get the most common element ((value, count) tuple)\n    most_common, count = counter.most_common(1)[0]\n    return most_common\n\n\n# Load dataset if not already loaded\nif 'test_dataset' not in globals():\n    from datasets import load_dataset\n    test_dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n\ntotal_examples = len(test_dataset)\ncorrect_count = 0\nstart_time = time.time()\n\n# Store failures for analysis\nfailures = []\n\nfor idx in tqdm(range(total_examples), desc=\"Voting\"):\n    example = test_dataset[idx]\n    question = example[\"question\"]\n    \n    # Get Ground Truth\n    ground_truth_raw = extract_hash_answer(example[\"answer\"])\n    ground_truth_norm = normalize_answer(ground_truth_raw)\n\n    # Prepare Prompt\n    prompt = generate_inference_prompt(question)\n    \n    # Create Batch: Replicate the prompt VOTE_SAMPLES times\n    # This sends 8 identical prompts to the model at once\n    batch_prompts = [prompt] * VOTE_SAMPLES\n\n    try:\n        # Generate samples in parallel\n        sampler_output = generation_sampler(\n            input_strings=batch_prompts,\n            max_generation_steps=MAX_GEN_STEPS,\n            temperature=TEMPERATURE,\n            top_k=40, # Allow diversity for voting\n        )\n        \n        # Extract answers from all samples\n        candidates = []\n        for response_text in sampler_output.text:\n            # Cleanup stop tokens\n            if \"<end_of_turn>\" in response_text:\n                response_text = response_text.split(\"<end_of_turn>\")[0]\n            \n            # Extract\n            raw_ans = extract_answer_robust(response_text)\n            norm_ans = normalize_answer(raw_ans)\n            candidates.append(norm_ans)\n            \n        # Perform Majority Vote\n        final_prediction = get_majority_vote(candidates)\n        \n        # Check Correctness\n        is_correct = False\n        if final_prediction is not None and ground_truth_norm is not None:\n            try:\n                is_correct = float(final_prediction) == float(ground_truth_norm)\n            except ValueError:\n                is_correct = final_prediction == ground_truth_norm\n        \n        if is_correct:\n            correct_count += 1\n        else:\n            # Log failure for inspection\n            failures.append({\n                \"q\": question,\n                \"gt\": ground_truth_norm,\n                \"pred\": final_prediction,\n                \"candidates\": candidates\n            })\n\n    except Exception as e:\n        print(f\"Error on example {idx}: {e}\")\n\nend_time = time.time()\ntotal_time = end_time - start_time\n\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"MAJORITY VOTING RESULTS\")\nprint(\"=\"*60)\nprint(f\"Total Time: {total_time:.1f}s ({total_time/total_examples:.2f}s per question)\")\nprint(f\"Samples per Question: {VOTE_SAMPLES}\")\nprint(\"-\" * 60)\nprint(f\"Final Accuracy: {correct_count}/{total_examples} ({100*correct_count/total_examples:.2f}%)\")\nprint(\"=\"*60)\n\n# Show a sample failure to see voting behavior\nif failures:\n    print(\"\\nSample Failure (Voting Analysis):\")\n    f = failures[0]\n    print(f\"Question: {f['q'][:100]}...\")\n    print(f\"Ground Truth: {f['gt']}\")\n    print(f\"Voted Prediction: {f['pred']}\")\n    print(f\"Vote Distribution: {f['candidates']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T21:13:19.527669Z","iopub.execute_input":"2025-12-14T21:13:19.527854Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nEvaluating with Majority Voting (k=1)\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac8e72fd234b447b9cf6396794910ccf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"main/train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4e0365be8ed4c8bbf10f2cdff990b0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"main/test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9095dc40300e494984e8fd821c9f57e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7223b538339a4bec86468198737cf6a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03c8961a2dd4469b99b2ae12d5a77809"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Voting:   0%|          | 0/1319 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1a27053e51d458fb0de4d01853724e3"}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"### Export SFT checkpoints as a zip & clean up\n\nThe SFT trainer writes a full Tunix checkpoint tree under `CHECKPOINT_DIR` and TensorBoard\nlogs under `TENSORBOARD_DIR`. To keep the number of files small and make it easy to download\nthe weights, we:\n\n1. Zip **only** the SFT checkpoint tree into a single archive.\n2. Remove the original checkpoint and TensorBoard directories (they can always be recreated by re‚Äërunning SFT).\n\n> **Note** ‚Äì This step assumes that SFT training has already run and produced at least one checkpoint.","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\n\nprint(\"Zipping SFT checkpoints and cleaning up SFT artifacts...\")\n\nif \"CHECKPOINT_DIR\" not in globals():\n    print(\"  ! CHECKPOINT_DIR not defined; did you run the SFT config cell?\")\nelse:\n    if os.path.isdir(CHECKPOINT_DIR):\n        zip_base = \"tunix_sft_dual_stream_gemma3_actor_ckpt\"\n        zip_path = shutil.make_archive(zip_base, \"zip\", CHECKPOINT_DIR)\n        print(f\"  ‚úì Created SFT zip archive: {zip_path}\")\n       # shutil.rmtree(CHECKPOINT_DIR)\n        #print(\"  ‚úì Removed SFT checkpoint directory:\", CHECKPOINT_DIR)\n    else:\n        print(\"  ! No SFT checkpoint dir found at:\", CHECKPOINT_DIR)\n\nif \"TENSORBOARD_DIR\" in globals() and os.path.isdir(TENSORBOARD_DIR):\n    #shutil.rmtree(TENSORBOARD_DIR)\n    print(\"  ‚úì Removed SFT TensorBoard log directory:\", TENSORBOARD_DIR)\n\n#print(\"SFT artifact cleanup complete.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Part 2 ‚Äî GRPO with DSA‚ÄëCAST Rewards (Reinforcement Learning)\n\nThis section is your original **DSA‚ÄëCAST + Tunix GRPO notebook**, embedded after SFT.\n\nAt a high level, it does:\n\n1. **Environment & data setup**\n   - Logs in to Hugging Face (via Kaggle secret).\n   - Ensures JAX + Tunix are installed on the TPU.\n   - Loads GSM8K from TFDS or a Kaggle dataset into a rollout‚Äëfriendly format:\n     - each example has a `prompts` field already formatted with the Dual‚ÄëStream template\n     - plus `question` and `answer` fields used by the reward functions.\n\n2. **Reward design (DSA‚ÄëCAST)**\n   - `reward_format_exact`: strict regex check for the full `<reasoning>...<answer>...` layout.\n   - `reward_format_soft`: softer ‚Äútag hygiene‚Äù score that penalizes missing or repeated tags.\n   - `reward_cast_math_and_completeness`: CAST‚Äëstyle scoring of:\n     - math accuracy,\n     - solution completeness,\n     - plus an extra format bonus.\n\n3. **GRPO training loop**\n   - Builds a Tunix `RLCluster` with:\n     - an **actor model** (the policy we update) and\n     - a **reference model** (kept frozen).\n   - Uses `GRPOLearner` to:\n     1. Sample `NUM_GENERATIONS` rollouts per prompt.\n     2. Score those rollouts with the DSA‚ÄëCAST reward.\n     3. Apply GRPO updates to the actor, keeping the reference fixed.\n\n4. **Baseline & post‚ÄëGRPO evaluation**\n   - Evaluate the base Gemma 3 1B‚ÄëIT model (pre‚ÄëGRPO) on GSM8K.\n   - Evaluate the GRPO‚Äëtrained actor on the same test data.\n   - Compare accuracy, ‚Äúpartial credit‚Äù, and format‚Äëadherence metrics.\n\n5. **Export & cleanup**\n   - Zip the **best actor checkpoint** into a single file:\n     - `tunix_dsa_cast_grpo_actor_ckpt.zip`\n   - Remove the GRPO checkpoint tree to keep Kaggle‚Äôs output under its file limits.","metadata":{}},{"cell_type":"markdown","source":"# DSA-CAST + Tunix GRPO on Gemma3-1B (TPU, Kaggle)\n\nThis notebook:\n\n1. Sets up **Gemma3-1B-IT** on a Kaggle TPU using **Tunix**.\n2. Uses the `<reasoning> ... </reasoning>` and `<answer> ... </answer>` format for math problems (GSM8K-style).\n3. Defines a **CAST-style reward** that strongly favors:\n   - mathematical accuracy, and  \n   - answer completeness & proper tagging.\n4. Runs a **Tunix GRPO** reinforcement learning loop using that reward.\n5. Saves the final **Tunix checkpoint (no safetensors export)** so it can be re-used in another notebook.","metadata":{}},{"cell_type":"code","source":"# HF Hub login removed\n#\n# GRPO now reuses the Gemma 3 model and tokenizer loaded in the SFT section\n# via kagglehub + Tunix. No Hugging Face access token or secrets are needed\n# anywhere in this notebook.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# (Intentionally left simple)\n#\n# This cell used to log in to Hugging Face with a hard‚Äëcoded token.\n# We no longer do that ‚Äî the model weights and tokenizer are loaded\n# once in the SFT section via Kaggle assets and reused for GRPO.\npass","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Environment setup: JAX TPU + Tunix (no git+ installs) ===\nimport os\n\n# Make sure JAX uses TPU and has full memory\nos.environ.setdefault(\"JAX_PLATFORMS\", \"tpu,cpu\")\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"\n\n# JAX TPU build\n!pip install -q \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n\n# Tunix from PyPI (recommended), plus other deps\n!pip install -q google-tunix[prod] humanize datasets tensorflow_datasets kagglehub huggingface_hub\n\n# If you *still* want Qwix-based LoRA, use the PyPI wheel instead of git:\n# (no GitHub username prompt; it just pulls the published wheel)\n\nprint(\"Environment installs complete (no git+).\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Imports & global configuration ===\nimport functools\nimport json\nimport re\nimport shutil\nimport sys\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional, Tuple\nimport random\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\nimport optax\nimport humanize\nimport sympy as sp\n\n# Dual-Stream tags\nREASONING_START = \"<reasoning>\"\nREASONING_END = \"</reasoning>\"\nANSWER_START = \"<answer>\"\nANSWER_END = \"</answer>\"\n\n# Monologue section headings\nPLAN_HEADING = \"Plan:\"\nREASONING_HEADING = \"Reasoning:\"\nEVIDENCE_HEADING = \"Evidence:\"\nSANITY_HEADING = \"Sanity_check:\"\n\nSYSTEM_PROMPT = f\"\"\"You are a careful math tutor. You MUST respond in a Dual‚ÄëStream format.\n\nInside the {REASONING_START}...{REASONING_END} block, always structure your thoughts into four sections:\n\n1. Plan: high‚Äëlevel steps you will take to solve the problem.\n2. Evidence: citations, calculations, or explicit checks that support the reasoning.\n3. Reasoning: detailed step‚Äëby‚Äëstep execution.\n\nAfter the reasoning block, put ONLY the final numeric result inside {ANSWER_START}...{ANSWER_END}.\nThe final answer must appear exactly once in {ANSWER_START}...{ANSWER_END}.\n\"\"\".strip()\n\nTEMPLATE = \"\"\"<start_of_turn>user\n{system_prompt}\n\n{question}<end_of_turn>\n<start_of_turn>model\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Hyperparameters ===\n\nMODEL_ID = \"google/gemma-3-1b-it\"\n\nTRAIN_DATA_DIR = \"./data/gsm8k_train\"\nTEST_DATA_DIR = \"./data/gsm8k_test\"\n\nNUM_TPUS = len(jax.devices())\nif NUM_TPUS == 8:\n    MESH_COUNTS = (1, 4)\nelif NUM_TPUS == 1:\n    MESH_COUNTS = (1, 1)\nelse:\n    raise ValueError(f\"Unsupported number of TPU devices: {NUM_TPUS}\")\n\nMESH = [MESH_COUNTS, (\"fsdp\", \"tp\")]\n\nMAX_PROMPT_LENGTH = 256\nTOTAL_GENERATION_STEPS = 384\nTEMPERATURE = 0.9\nTOP_P = 1.0\nTOP_K = 50\nNUM_GENERATIONS = 2\nNUM_ITERATIONS = 1\n\nTRAIN_MICRO_BATCH_SIZE = 1\nNUM_BATCHES = 256\nTRAIN_FRACTION = 0.9\nNUM_EPOCHS = 1\n\nMAX_STEPS = 1500  # DEBUG: cap GRPO training steps for a quick run\n\nLEARNING_RATE = 3e-6\nB1 = 0.9\nB2 = 0.99\nWEIGHT_DECAY = 0.1\nWARMUP_STEPS = int(0.1 * MAX_STEPS)\nMAX_GRAD_NORM = 0.1\n\nCKPT_DIR = \"/kaggle/working/grpo_ckpts\"\nSAVE_INTERVAL_STEPS = 200\nMAX_TO_KEEP = 4\n\nGENERATION_CONFIGS = {\n    \"greedy\":   {\"temperature\": None, \"top_k\": 1,   \"top_p\": None},\n    \"standard\": {\"temperature\": 0.7,  \"top_k\": 50,  \"top_p\": 0.95},\n    \"liberal\":  {\"temperature\": 0.85, \"top_k\": 2000,\"top_p\": 1.0},\n}\n\nprint(\"Hyperparameters set. MAX_STEPS =\", MAX_STEPS)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Data preprocessing: GSM8K via TFDS ===\n\nimport tensorflow_datasets as tfds\n\ndef extract_hash_answer(text: str) -> Optional[str]:\n    if \"####\" not in text:\n        return None\n    return text.split(\"####\", 1)[1].strip()\n\ndef _load_gsm8k_tfds(data_dir: str, split: str):\n    import tensorflow_datasets.text.gsm8k\n    return tfds.data_source(\n        \"gsm8k\",\n        split=split,\n        data_dir=data_dir,\n        builder_kwargs={\"file_format\": tfds.core.FileFormat.ARRAY_RECORD},\n        download=True,\n    )\n\ndef get_gsm8k_dataset(data_dir: str, split: str = \"train\") -> grain.MapDataset:\n    os.makedirs(data_dir, exist_ok=True)\n    ds = _load_gsm8k_tfds(data_dir, split)\n\n    def _as_text(v):\n        return v if isinstance(v, str) else v.decode(\"utf-8\")\n\n    dataset = (\n        grain.MapDataset.source(ds)\n        .shuffle(seed=42)\n        .map(\n            lambda x: {\n                \"prompts\": TEMPLATE.format(\n                    system_prompt=SYSTEM_PROMPT,\n                    question=_as_text(x[\"question\"]),\n                ),\n                \"question\": _as_text(x[\"question\"]),\n                \"answer\": extract_hash_answer(_as_text(x[\"answer\"])),\n            }\n        )\n    )\n    return dataset\n\ntrain_raw = get_gsm8k_dataset(TRAIN_DATA_DIR, split=\"train\")\ntest_raw = get_gsm8k_dataset(TEST_DATA_DIR, split=\"test\")\n\ntrain_dataset = train_raw.batch(TRAIN_MICRO_BATCH_SIZE)[:NUM_BATCHES]\n\nif TRAIN_FRACTION == 1.0:\n    train_dataset = train_dataset.repeat(NUM_EPOCHS)\n    val_dataset = None\nelse:\n    cutoff = int(len(train_dataset) * TRAIN_FRACTION)\n    train_dataset = train_dataset[:cutoff].repeat(NUM_EPOCHS)\n    val_dataset = train_dataset[cutoff:].repeat(NUM_EPOCHS) if cutoff < len(train_dataset) else None\n\nNUM_TEST_BATCHES = 64\ntest_dataset = test_raw.batch(TRAIN_MICRO_BATCH_SIZE)[:NUM_TEST_BATCHES]\n\nprint(\"Dataset sizes (batches):\",\n      len(train_dataset),\n      0 if val_dataset is None else len(val_dataset),\n      len(test_dataset))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Utility: TPU memory usage ===\ndef show_hbm_usage():\n    fmt = functools.partial(humanize.naturalsize, binary=True)\n    for d in jax.local_devices():\n        stats = d.memory_stats()\n        used = stats[\"bytes_in_use\"]\n        limit = stats[\"bytes_limit\"]\n        print(f\"Using {fmt(used)} / {fmt(limit)} ({used/limit:%}) on {d}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === GRPO actor/reference setup using SFT model (no HF Hub) ===\n#\n# Instead of downloading Gemma3-1B-IT again from Hugging Face and logging in\n# with a token, we REUSE the model that was trained during the SFT phase.\n#\n# - `gemma3_model` was created in the SFT section via:\n#     params_safetensors_lib.create_model_from_safe_tensors(local_model_path, model_config, mesh)\n#   and then fine-tuned with PeftTrainer.\n# - `tokenizer` and `model_config` and `mesh` were also created in SFT.\n#\n# Here:\n#   * `actor_model`  = the SFT‚Äëtrained Gemma3 model (trainable in GRPO).\n#   * `reference_model` = a fresh, frozen copy of the base Gemma3-1B-IT weights.\n#\n# This gives you a clean SFT ‚Üí GRPO pipeline with no Hugging Face Hub login\n# and no hard-coded API keys.\n\n# Make sure SFT has run\ntry:\n    gemma3_model\n    tokenizer\n    model_config\n    mesh\n    local_model_path\nexcept NameError as e:\n    raise RuntimeError(\n        \"SFT section must be run before GRPO. \"\n        \"Missing variable: {}\".format(e)\n    )\n\nprint(\"Reusing SFT-trained Gemma3 model as GRPO actor...\")\nactor_model = gemma3_model  # SFT fine-tuned weights\n\nprint(\"Loading frozen reference model from base Gemma3 checkpoint via Tunix...\")\nwith mesh:\n    reference_model = params_safetensors_lib.create_model_from_safe_tensors(\n        local_model_path,  # same directory used in SFT\n        model_config,\n        mesh,\n    )\n\n# EOS tokens: reuse tokenizer EOS id\nEOS_TOKENS = [tokenizer.eos_id()]\nprint(\"EOS token IDs:\", EOS_TOKENS)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === CAST-style helpers ===\n\ndef extract_final_number(text: str) -> Optional[str]:\n    if text is None:\n        return None\n\n    # Prefer numbers inside the <answer> ... </answer> block\n    m = re.search(r\"<answer>(.*?)</answer>\", text, flags=re.IGNORECASE | re.DOTALL)\n    segment = m.group(1) if m else text\n\n    # Try GSM8K-style '#### 42' first\n    m = re.search(r\"####\\s*([-+]?[0-9][0-9.,/]*)\", segment)\n    if m:\n        return m.group(1).replace(\",\", \"\").strip()\n\n    # Otherwise, grab the first reasonable-looking number\n    nums = re.findall(r\"[-+]?[0-9][0-9.,/]*\", segment)\n    if not nums:\n        return None\n    return nums[0].replace(\",\", \"\").strip()\n\n\ndef extract_calc_statements(text: str):\n    \"\"\"Extract CALC: expr = result statements from a completion.\n\n    Returns a list of (expr_str, result_str).\n    \"\"\"\n    if not text:\n        return []\n\n    calc_lines = []\n    for line in text.splitlines():\n        if \"CALC:\" not in line:\n            continue\n        # Expect patterns like 'CALC: expr = result'\n        m = re.search(r\"CALC:\\s*(.*?)=(.*)\", line)\n        if not m:\n            continue\n        expr_str = m.group(1).strip()\n        result_str = m.group(2).strip()\n        if expr_str and result_str:\n            calc_lines.append((expr_str, result_str))\n    return calc_lines\n\n\ndef calc_consistency_score(text: str) -> float:\n    \"\"\"Score how consistent CALC: statements are, using sympy.\n\n    - If there are no CALC statements, returns 0.0 (no signal).\n    - Otherwise, returns (# correct equations) / (# equations), in [0, 1].\n    \"\"\"\n    calcs = extract_calc_statements(text)\n    if not calcs:\n        return 0.0\n\n    correct = 0\n    total = 0\n    for expr_str, result_str in calcs:\n        try:\n            expr = sp.sympify(expr_str)\n            rhs = sp.sympify(result_str)\n            diff = sp.simplify(expr - rhs)\n            is_zero = bool(diff == 0)\n            correct += 1 if is_zero else 0\n            total += 1\n        except Exception:\n            # Parsing or evaluation failure counts as incorrect\n            total += 1\n            continue\n\n    if total == 0:\n        return 0.0\n    return float(correct) / float(total)\n\n    DSA-SFT=>GRPO-noLora-tunix\n    Version #27 with TPU v5e-8 Cancelled\n\n    36m\n    DSA-SFT=>GRPO-noLora-tunix\n    Interactive Session with TPU v5e-8 Running: 7m\n\n    40m\n\n    1\n\n    1 Active Event\n\ndef cast_style_scores(completions, answer):\n    \"\"\"Compute math accuracy, structural completeness, format bonus, and calc consistency.\n\n    Returns four lists (all floats):\n      - math_accs\n      - completeness\n      - format_bonus\n      - calc_consistency\n    \"\"\"\n    math_accs = []\n    completeness = []\n    format_bonus = []\n    calc_consistency = []\n\n    for text, gold in zip(completions, answer):\n        t = text or \"\"\n\n        # === Math accuracy ===\n        m_acc = 0.0\n        pred_str = extract_final_number(t)\n        if pred_str is not None and gold is not None:\n            try:\n                pred_val = float(str(pred_str).strip())\n                gold_val = float(str(gold).strip())\n                if pred_val == gold_val:\n                    m_acc = 1.0\n                else:\n                    if gold_val != 0:\n                        ratio = pred_val / gold_val\n                        if 0.9 <= ratio <= 1.1:\n                            m_acc = 0.5\n                        elif 0.8 <= ratio <= 1.2:\n                            m_acc = 0.25\n            except Exception:\n                m_acc = 0.0\n\n        # === Structural completeness ===\n        has_tags = (\n            REASONING_START in t\n            and REASONING_END in t\n            and ANSWER_START in t\n            and ANSWER_END in t\n        )\n\n        # Section presence and order\n        def _idx(h: str) -> int:\n            return t.find(h)\n\n        positions = {\n            \"plan\": _idx(PLAN_HEADING),\n            \"reasoning\": _idx(REASONING_HEADING),\n            \"evidence\": _idx(EVIDENCE_HEADING),\n            \"sanity\": _idx(SANITY_HEADING),\n        }\n        present = {k: (v != -1) for k, v in positions.items()}\n\n        # Presence score: +1 for each present, -1 for each missing, normalized\n        pres_raw = sum(1.0 if present[k] else -1.0 for k in positions.keys())\n        pres_score = (pres_raw / 4.0 + 1.0) / 2.0  # roughly map into [0,1]\n\n        # Order score: only if all present\n        order_score = 0.0\n        if all(present.values()):\n            idxs = [\n                positions[\"plan\"],\n                positions[\"reasoning\"],\n                positions[\"evidence\"],\n                positions[\"sanity\"],\n            ]\n            if idxs == sorted(idxs):\n                order_score = 1.0\n            else:\n                order_score = 0.0\n\n        # Reasoning length: non-empty body inside <reasoning>...</reasoning>\n        m_block = re.search(\n            rf\"{re.escape(REASONING_START)}(.*?){re.escape(REASONING_END)}\",\n            t,\n            flags=re.DOTALL | re.IGNORECASE,\n        )\n        reasoning_body = m_block.group(1) if m_block else \"\"\n        reasoning_len = len(reasoning_body.strip())\n        if reasoning_len > 0:\n            len_score = min(1.0, reasoning_len / 300.0)\n        else:\n            len_score = 0.0\n\n        c_score = max(0.0, (pres_score + order_score + len_score) / 3.0)\n\n        # === Format bonus ===\n        if has_tags and all(present.values()) and order_score > 0:\n            f_bonus = 1.0\n        elif has_tags:\n            f_bonus = 0.5\n        else:\n            f_bonus = 0.0\n\n        # === Evidence calc consistency ===\n        calc_score = calc_consistency_score(t)\n\n        math_accs.append(float(m_acc))\n        completeness.append(float(c_score))\n        format_bonus.append(float(f_bonus))\n        calc_consistency.append(float(calc_score))\n\n    return math_accs, completeness, format_bonus, calc_consistency","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### DSA‚ÄëCAST Reward Functions (What the RL Signal Is Measuring)\n\nThe next cell defines three core reward functions used by GRPO, all of which\nare aware of the **Plan / Reasoning / Evidence / Sanity_check** structure\ninside `<reasoning>...</reasoning>` as well as the outer `<answer>...</answer>` block.\n\n1. **`reward_format_exact`**  \n   - Uses a strict regular expression over the full completion.  \n   - Gives a high reward when the output looks like:\n\n     ```text\n     <reasoning>\n     Plan:\n       ...\n\n     Reasoning:\n       ...\n\n     Evidence:\n       ...\n\n     Sanity_check:\n       ...\n     </reasoning>\n     <answer>\n       ...single final scalar...\n     </answer>\n     ```\n\n   - Any major deviation (missing tags, missing headings, wrong order, multiple answer blocks, etc.) receives 0.\n\n2. **`reward_format_soft`**  \n   - Provides a smoother shaping signal when the model is ‚Äúon the way‚Äù to the desired format.  \n   - It:\n     - rewards the presence of `<reasoning>...</reasoning>` and `<answer>...</answer>` tags,\n     - rewards each of the four headings when present,\n     - adds extra reward when the headings appear in the correct order,\n     - and penalizes missing or badly ordered structure.\n\n3. **`reward_cast_math_and_completeness`**  \n   - Calls `cast_style_scores`, which:\n     - extracts the numeric answer from the `<answer> ... </answer>` block,\n     - compares it to the GSM8K ground‚Äëtruth answer (with some tolerance),\n     - and scores structural completeness based on:\n       - presence and order of Plan / Reasoning / Evidence / Sanity_check,\n       - and non‚Äëtrivial reasoning content inside `<reasoning>...</reasoning>`.\n   - Then combines:\n     - **math accuracy** (did we get the right number?),\n     - **completeness** (did we actually solve the problem with meaningful structure?), and\n     - **format bonus** (are we respecting Dual‚ÄëStream tags and headings?)\n     into a single scalar.\n\nDuring GRPO, all three rewards are **added together** to produce a single\nreward per sampled rollout. That reward is what drives the policy updates.\n\nIn practice, you can view DSA‚ÄëCAST here as a **grading rubric** for the DSA style:\nthe SFT stage teaches the model *how* to speak in that structure, and\nDSA‚ÄëCAST + GRPO teaches it to speak **better, more consistently, and more correctly**\nwhile keeping Plan / Reasoning / Evidence / Sanity_check intact.","metadata":{}},{"cell_type":"code","source":"# === Reward functions for Tunix GRPO ===\n\n# Strict overall format: <reasoning> (with sections) then <answer>, in order.\nsection_pattern = (\n    rf\"{re.escape(PLAN_HEADING)}.*?\"\n    rf\"{re.escape(REASONING_HEADING)}.*?\"\n    rf\"{re.escape(EVIDENCE_HEADING)}.*?\"\n    rf\"{re.escape(SANITY_HEADING)}\"\n)\n\nmatch_format = re.compile(\n    rf\"^[\\s]{{0,}}\"\n    rf\"{re.escape(REASONING_START)}.*?{section_pattern}.*?{re.escape(REASONING_END)}.*?\"\n    rf\"{re.escape(ANSWER_START)}(.+?){re.escape(ANSWER_END)}\"\n    rf\"[\\s]{{0,}}$\",\n    flags=re.MULTILINE | re.DOTALL,\n)\n\ndef reward_format_exact(prompts, completions, **kwargs):\n    \"\"\"High reward only when we see the full DSA structure and dual-stream tags.\n\n    +3.0 if:\n      - <reasoning>...</reasoning> and <answer>...</answer> are present in order, and\n      - all four headings (Plan / Reasoning / Evidence / Sanity_check) appear in order inside <reasoning>.\n\n    0.0 otherwise.\n    \"\"\"\n    scores = []\n    for resp in completions:\n        ok = bool(match_format.search(resp or \"\"))\n        scores.append(3.0 if ok else 0.0)\n    return scores\n\n\ndef reward_format_soft(prompts, completions, **kwargs):\n    \"\"\"Softer shaping reward for partial formatting progress.\n\n    Rewards:\n      - presence of <reasoning>/<answer> tags,\n      - presence of each heading,\n      - correct ordering of the headings.\n\n    Penalties when tags or headings are missing or badly ordered.\n    \"\"\"\n    scores = []\n    for resp in completions:\n        t = resp or \"\"\n        r = 0.0\n\n        # Dual-stream tags\n        has_reasoning = REASONING_START in t and REASONING_END in t\n        has_answer = ANSWER_START in t and ANSWER_END in t\n        r += 1.0 if has_reasoning else -1.0\n        r += 1.0 if has_answer else -1.0\n\n        # Heading presence\n        def _idx(h: str) -> int:\n            return t.find(h)\n\n        positions = {\n            \"plan\": _idx(PLAN_HEADING),\n            \"reasoning\": _idx(REASONING_HEADING),\n            \"evidence\": _idx(EVIDENCE_HEADING),\n            \"sanity\": _idx(SANITY_HEADING),\n        }\n        present = {k: (v != -1) for k, v in positions.items()}\n        for k, is_present in present.items():\n            r += 0.75 if is_present else -0.75\n\n        # Heading order\n        if all(present.values()):\n            idxs = [\n                positions[\"plan\"],\n                positions[\"reasoning\"],\n                positions[\"evidence\"],\n                positions[\"sanity\"],\n            ]\n            if idxs == sorted(idxs):\n                r += 1.0\n            else:\n                r -= 1.0\n\n        scores.append(r)\n    return scores\n\n\ndef reward_cast_math_and_completeness(prompts, completions, answer, **kwargs):\n    \"\"\"CAST-style reward: math accuracy + structural completeness + format + calc consistency.\n\n    The combination is:\n      R = 3 * math_accuracy + 2 * completeness + 1 * format_bonus + 1 * calc_consistency\n    where each term is in roughly [0, 1].\n    \"\"\"\n    math_accs, completeness, fbonus, calc_consistency = cast_style_scores(completions, answer)\n    scores = []\n    for ma, c, fb, cc in zip(math_accs, completeness, fbonus, calc_consistency):\n        scores.append(3.0 * ma + 2.0 * c + 1.0 * fb + 1.0 * cc)\n    return scores\n\nprint(\"Reward functions defined.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Evaluation utilities ===\n\ndef build_sampler(policy_model, tokenizer, model_config):\n    return sampler_lib.Sampler(\n        transformer=policy_model,\n        tokenizer=tokenizer,\n        cache_config=sampler_lib.CacheConfig(\n            cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n            num_layers=model_config.num_layers,\n            num_kv_heads=model_config.num_kv_heads,\n            head_dim=model_config.head_dim,\n        ),\n    )\n\n\ndef generate_answers(questions, sampler, temperature=0.7, top_k=50, top_p=0.95, seed=None):\n    if isinstance(questions, str):\n        batch = [\n            TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=questions),\n        ]\n    else:\n        batch = [\n            TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=q)\n            for q in questions\n        ]\n    out = sampler(\n        input_strings=batch,\n        max_generation_steps=TOTAL_GENERATION_STEPS,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        echo=False,\n        seed=seed,\n        eos_tokens=EOS_TOKENS,\n    )\n    texts = out.text\n    return texts[0] if isinstance(questions, str) else texts\n\n\ndef evaluate_dataset(dataset, sampler, num_passes=1):\n    total = 0\n    strict_correct = 0\n    approx_correct = 0\n    format_ok = 0\n\n    for batch in dataset:\n        questions = batch[\"question\"]\n        answers = batch[\"answer\"]\n        multiple_outputs = [[] for _ in range(len(questions))]\n\n        for s in range(num_passes):\n            responses = generate_answers(\n                questions,\n                sampler,\n                temperature=GENERATION_CONFIGS[\"greedy\"][\"temperature\"],\n                top_k=GENERATION_CONFIGS[\"greedy\"][\"top_k\"],\n                top_p=GENERATION_CONFIGS[\"greedy\"][\"top_p\"],\n                seed=s,\n            )\n            for idx, resp in enumerate(responses):\n                multiple_outputs[idx].append(resp)\n\n        for q, a, resp_list in zip(questions, answers, multiple_outputs):\n            is_correct = False\n            is_approx = False\n            has_format = False\n            for resp in resp_list:\n                if match_format.search(resp or \"\") is not None:\n                    has_format = True\n                guess = extract_final_number(resp or \"\")\n                truth = extract_final_number(a or \"\")\n                try:\n                    if truth is not None and guess is not None:\n                        g = float(guess)\n                        t = float(truth)\n                        if g == t:\n                            is_correct = True\n                        ratio = g / t if t != 0 else 0.0\n                        if 0.9 <= ratio <= 1.1:\n                            is_approx = True\n                except Exception:\n                    pass\n                if is_correct and is_approx and has_format:\n                    break\n\n            total += 1\n            if is_correct:\n                strict_correct += 1\n            if is_approx:\n                approx_correct += 1\n            if has_format:\n                format_ok += 1\n\n    acc = 100.0 * strict_correct / max(1, total)\n    approx_acc = 100.0 * approx_correct / max(1, total)\n    fmt_acc = 100.0 * format_ok / max(1, total)\n\n    print(f\"Total examples: {total}\")\n    print(f\"Strict accuracy: {acc:.2f}%\")\n    print(f\"Approx accuracy: {approx_acc:.2f}%\")\n    print(f\"Format accuracy: {fmt_acc:.2f}%\")\n    return dict(\n        total=total,\n        strict_accuracy=acc,\n        approx_accuracy=approx_acc,\n        format_accuracy=fmt_acc,\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Baseline evaluation before GRPO ===\n\nbaseline_sampler = build_sampler(actor_model, tokenizer, model_config)\nprint(\"Evaluating baseline policy on a small test subset...\")\nbaseline_metrics = evaluate_dataset(test_dataset, baseline_sampler, num_passes=1)\nbaseline_metrics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === RLCluster, optimizer, and GRPOLearner setup ===\n\nckpt_options = ocp.CheckpointManagerOptions(\n    save_interval_steps=SAVE_INTERVAL_STEPS,\n    max_to_keep=MAX_TO_KEEP,\n)\n\nschedule = optax.schedules.warmup_cosine_decay_schedule(\n    init_value=0.0,\n    peak_value=LEARNING_RATE,\n    warmup_steps=WARMUP_STEPS,\n    decay_steps=MAX_STEPS,\n    end_value=0.0,\n)\noptimizer = optax.adamw(\n    learning_rate=schedule,\n    b1=B1,\n    b2=B2,\n    weight_decay=WEIGHT_DECAY,\n)\nif MAX_GRAD_NORM is not None:\n    optimizer = optax.chain(\n        optax.clip_by_global_norm(MAX_GRAD_NORM),\n        optimizer,\n    )\n\ncluster_config = rl_cluster_lib.ClusterConfig(\n    role_to_mesh={\n        rl_cluster_lib.Role.ACTOR: mesh,\n        rl_cluster_lib.Role.REFERENCE: mesh,\n        rl_cluster_lib.Role.ROLLOUT: mesh,\n    },\n    rollout_engine=\"vanilla\",\n    offload_to_cpu=False,\n    training_config=rl_cluster_lib.RLTrainingConfig(\n        actor_optimizer=optimizer,\n        eval_every_n_steps=64,\n        max_steps=MAX_STEPS,\n        mini_batch_size=TRAIN_MICRO_BATCH_SIZE,\n        train_micro_batch_size=TRAIN_MICRO_BATCH_SIZE,\n        metrics_logging_options=None\n        checkpoint_root_directory=CKPT_DIR,\n        checkpointing_options=ckpt_options,\n    ),\n    rollout_config=base_rollout.RolloutConfig(\n        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n        max_prompt_length=MAX_PROMPT_LENGTH,\n        kv_cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n        temperature=TEMPERATURE,\n        top_p=TOP_P,\n        top_k=TOP_K,\n        eos_tokens=EOS_TOKENS,\n    ),\n)\n\ngrpo_config = GRPOConfig(\n    num_generations=NUM_GENERATIONS,\n    num_iterations=NUM_ITERATIONS,\n    beta=0.08,\n    epsilon=0.2,\n)\n\nrl_cluster = rl_cluster_lib.RLCluster(\n    actor=actor_model,\n    reference=reference_model,\n    tokenizer=tokenizer,\n    cluster_config=cluster_config,\n)\n\ngrpo_trainer = GRPOLearner(\n    rl_cluster=rl_cluster,\n    reward_fns=[\n        reward_format_exact,\n        reward_format_soft,\n        reward_cast_math_and_completeness,\n    ],\n    grpo_config=grpo_config,\n)\n\nprint(\"RLCluster and GRPOLearner ready.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Run GRPO training ===\n\nwith mesh:\n    show_hbm_usage()\n    grpo_trainer.train(train_dataset, val_dataset)\n\nprint(\"GRPO training complete.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Load final trained params & re-evaluate ===\n\ntrained_ckpt_path = os.path.join(\n    CKPT_DIR, \"actor\", str(MAX_STEPS), \"model_params\"\n)\n\nfinetuned_sampler = build_sampler(actor_model, tokenizer, model_config)\nprint(\"Evaluating finetuned policy on test subset...\")\nfinetuned_metrics = evaluate_dataset(test_dataset, finetuned_sampler, num_passes=1)\nfinetuned_metrics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Export final Tunix checkpoint as a single zip and clean up ===\nimport os\nimport shutil\n\n# Tunix checkpoint root (matches what we used in the training config)\nCKPT_DIR = \"/kaggle/working/grpo_ckpts\"\n\nactor_root = os.path.join(CKPT_DIR, \"actor\")\nif not os.path.exists(actor_root):\n    raise FileNotFoundError(f\"Actor checkpoint dir not found: {actor_root}\")\n\n# Find the most recent actor step directory (they're named by step number)\nstep_dirs = [\n    d for d in os.listdir(actor_root)\n    if os.path.isdir(os.path.join(actor_root, d)) and d.isdigit()\n]\nif not step_dirs:\n    raise RuntimeError(f\"No step subdirs found in {actor_root}\")\n\nbest_step = max(step_dirs, key=lambda s: int(s))\nactor_step_dir = os.path.join(actor_root, best_step)\nprint(\"Using actor checkpoint step:\", best_step)\nprint(\"Directory:\", actor_step_dir)\n\n# 1) Zip just that actor step directory\nzip_base = \"tunix_dsa_cast_grpo_actor_ckpt\"\nzip_path = shutil.make_archive(zip_base, \"zip\", actor_step_dir)\nprint(f\"\\nCreated zip archive: {zip_path}\")\n\n# 2) Remove the full GRPO checkpoint tree to stay under Kaggle's file limit\nif os.path.exists(CKPT_DIR):\n    shutil.rmtree(CKPT_DIR)\n    print(f\"Removed training checkpoint directory: {CKPT_DIR}\")\n\nprint(\"\\nRemaining important artifact:\")\nprint(\"  -\", zip_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}