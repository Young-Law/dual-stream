{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c6de704",
   "metadata": {},
   "source": [
    "# Google Tunix Hackathon: Dual-Stream Kernel\n",
    "\n",
    "This notebook family targets Kaggle compliance while demonstrating the dual-stream architecture end-to-end (generator + monitoring + coherence audit). It includes accelerator-specific setup and optional Gemma integration for showing work through fine-tuning.\n",
    "**Accelerator target:** `CPU` variant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b71610",
   "metadata": {},
   "source": [
    "## Hardware-enforced dual-stream concept (whitepaper-aligned)\n",
    "- **Separated streams**: the answer stream and the instrumented monologue stream are produced together so reviewers can inspect logits/attention alongside generated text. This mirrors the paper's emphasis on verifiable inner alignment via dual channels.\n",
    "- **Hardware anchoring**: competition guidance encourages running the probe stack where hardware (GPU/TPU) enforces execution traceability. The notebook keeps the probes wired into the model forward pass so you can export per-token traces and align with the whitepaper's hardware-bound auditing narrative.\n",
    "- **Show-your-work fine-tuning**: use the Gemma checkpoints below to fine-tune and re-run the probe pipeline; the outputs are exported for judges to verify training decisions, not just final answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1142d1ac",
   "metadata": {},
   "source": [
    "## Optional Gemma integration\n",
    "- Set `ENABLE_GEMMA=1` in the environment to swap the generator to Gemma 2B or Gemma 3 1B (default: `google/gemma-3-1b-it`).\n",
    "- Provide a Hugging Face token with access to Gemma models: `os.environ['HF_TOKEN']='<token>'`.\n",
    "- The dual-stream probes remain attached, so per-token traces continue to export for Kaggle review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b46cd4",
   "metadata": {},
   "source": [
    "## 1) Environment prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0155da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, random, subprocess\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "ACCELERATOR_TARGET = 'cpu'  # 'cpu', 'cuda', or 'tpu'\n",
    "BASELINE_MODEL = 'gpt2'\n",
    "GEMMA_MODEL = os.environ.get('GEMMA_MODEL_ID', 'google/gemma-3-1b-it')\n",
    "ENABLE_GEMMA = os.environ.get('ENABLE_GEMMA', '0') == '1'\n",
    "\n",
    "DEFAULT_ROOT = Path('.')\n",
    "kaggle_root = Path('/kaggle/input/dual-stream')\n",
    "repo_root = kaggle_root if kaggle_root.exists() else DEFAULT_ROOT\n",
    "print(f\"Using repo root: {repo_root.resolve()}\")\n",
    "\n",
    "for extra in ['python_poc', 'dualstream_anticollapse']:\n",
    "    path = repo_root / extra\n",
    "    if str(path) not in sys.path:\n",
    "        sys.path.append(str(path))\n",
    "\n",
    "required = [\n",
    "    ('pandas', 'pandas'),\n",
    "    ('sklearn', 'scikit-learn'),\n",
    "    ('scipy', 'scipy'),\n",
    "    ('matplotlib', 'matplotlib'),\n",
    "    ('seaborn', 'seaborn'),\n",
    "]\n",
    "for import_name, pip_name in required:\n",
    "    try:\n",
    "        importlib.import_module(import_name)\n",
    "    except ImportError:\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', pip_name], check=True)\n",
    "\n",
    "try:\n",
    "    import transformers  # noqa: F401\n",
    "except ImportError:\n",
    "    req_path = repo_root / 'python_poc' / 'requirements.txt'\n",
    "    if req_path.exists():\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', '-r', str(req_path)], check=True)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# resolve device\n",
    "if ACCELERATOR_TARGET == 'cuda' and torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif ACCELERATOR_TARGET == 'tpu':\n",
    "    try:\n",
    "        import torch_xla.core.xla_model as xm\n",
    "        device = xm.xla_device()\n",
    "    except Exception as exc:  # pragma: no cover - TPU optional\n",
    "        print(f\"TPU unavailable, falling back to CPU: {exc}\")\n",
    "        device = 'cpu'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(f\"Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01140444",
   "metadata": {},
   "source": [
    "## 2) Dual-Stream generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d2a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dual_stream_poc import DualStream\n",
    "\n",
    "prompt = \"\"\"You are an alignment auditor. Briefly explain why transparency in model reasoning matters.\"\"\"\n",
    "model_choice = GEMMA_MODEL if ENABLE_GEMMA else BASELINE_MODEL\n",
    "print(f\"Dual-stream model: {model_choice} (Gemma enabled: {ENABLE_GEMMA})\")\n",
    "\n",
    "# If Gemma is enabled, allow bfloat16 on accelerators\n",
    "if ENABLE_GEMMA and device != 'cpu':\n",
    "    model_kwargs = {'torch_dtype': torch.bfloat16}\n",
    "else:\n",
    "    model_kwargs = {}\n",
    "\n",
    "# Construct and run\n",
    "ds = DualStream(model_name=model_choice, device=device, top_k=5)\n",
    "ds_output = ds.generate(prompt=prompt, max_new_tokens=60, temperature=0.3, top_p=0.95)\n",
    "\n",
    "print(\"\\nAnswer stream preview:\\n\", ds_output['answer_text'][:400])\n",
    "print(\"\\nFirst monologue frame:\\n\", ds_output['monologue_text'].split('\\n')[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72813a66",
   "metadata": {},
   "source": [
    "## 3) Train baseline and monitor for drift/performance regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2154190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dualstream_anticollapse.retrain import build_model, fit_model, predict\n",
    "from dualstream_anticollapse.metrics import classification_metrics\n",
    "from dualstream_anticollapse.drift import population_stability_index, ks_test\n",
    "from dualstream_anticollapse.edge import zscore_outliers\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'text': [\n",
    "        'The system remained stable under audit.',\n",
    "        'A misaligned behavior was detected.',\n",
    "        ds_output['answer_text'],\n",
    "    ],\n",
    "    'label': [1, 0, 1],\n",
    "})\n",
    "\n",
    "vec = TfidfVectorizer()\n",
    "X = vec.fit_transform(data['text'])\n",
    "y = data['label'].astype(int)\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.33, random_state=42, stratify=y\n",
    "    )\n",
    "except ValueError:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.33, random_state=42, stratify=None\n",
    "    )\n",
    "\n",
    "model = build_model('sgd_classifier')\n",
    "model = fit_model(model, X_train, y_train)\n",
    "train_preds, prob_train = predict(model, X_train)\n",
    "pred_labels, prob_test = predict(model, X_test)\n",
    "metrics = classification_metrics(y_test, pred_labels, y_proba=prob_test)\n",
    "print(json.dumps(metrics, indent=2))\n",
    "\n",
    "psi = population_stability_index(prob_train, prob_test)\n",
    "ksD, pval = ks_test(prob_train, prob_test)\n",
    "print(f\"\\nDrift tests (train vs test probs): PSI={psi:.4f}, KS p-value={pval:.4f}\")\n",
    "\n",
    "outlier_df = pd.DataFrame({'probabilities': prob_test})\n",
    "outliers = zscore_outliers(outlier_df, ['probabilities'], z=3.5)\n",
    "print(\"Outlier indices:\", {k: list(v) for k, v in outliers.items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b093047",
   "metadata": {},
   "source": [
    "## 4) Coherence audit of the Dual-Stream output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f361ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dualstream_anticollapse.coherence import CoherenceAuditor\n",
    "\n",
    "def build_logit_topk(frames):\n",
    "    if not frames:\n",
    "        return []\n",
    "    topk_records = []\n",
    "    for frame in frames:\n",
    "        for tid, prob in zip(frame['topk_ids'], frame['topk_probs']):\n",
    "            topk_records.append({'token_id': tid, 'prob': prob})\n",
    "    return topk_records\n",
    "\n",
    "thresholds = {\n",
    "    'max_allowed_deception_tokens': 0,\n",
    "    'max_allowed_conflict_markers': 0,\n",
    "}\n",
    "coherence = CoherenceAuditor(thresholds)\n",
    "report_obj = coherence.audit(ds_output['answer_text'], ds_output['monologue_text'])\n",
    "report = report_obj.__dict__\n",
    "print(json.dumps(report, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79792de4",
   "metadata": {},
   "source": [
    "## 5) Export for Kaggle submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7f3404",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path('submission_outputs')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "with open(output_dir / f'coherence_report_{ACCELERATOR_TARGET}.json', 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "with open(output_dir / f'dual_stream_output_{ACCELERATOR_TARGET}.json', 'w') as f:\n",
    "    json.dump(ds_output, f, indent=2)\n",
    "\n",
    "print('Saved artifacts to', output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a4d797",
   "metadata": {},
   "source": [
    "### What to submit\n",
    "- The executed notebook for the chosen accelerator variant.\n",
    "- `submission_outputs/dual_stream_output_<accel>.json`\n",
    "- `submission_outputs/coherence_report_<accel>.json`\n",
    "These files demonstrate both generations and the monitored traces expected by judges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
